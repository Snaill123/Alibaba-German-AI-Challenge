{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2层卷积+2层分类，50k数据集训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_data : shuffle 50k\n",
    "* vali_data : 5k\n",
    "* batch_size : 1000\n",
    "* Optimizer : SGD\n",
    "* learning_rate : 1e-5\n",
    "* accuracy_threshold : 0.98\n",
    "* step_limit : 64K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "The shape of vali is  (5000, 18449)\n",
      "step 0, train accuracy 0.064, train loss 10902.4\n",
      "step 100, train accuracy 0.268, train loss 2868.05\n",
      "step 200, train accuracy 0.342, train loss 2351.02\n",
      "step 300, train accuracy 0.331, train loss 2147.67\n",
      "step 400, train accuracy 0.4, train loss 1917.14\n",
      "step 500, train accuracy 0.401, train loss 1897.01\n",
      "step 600, train accuracy 0.407, train loss 1860.57\n",
      "step 700, train accuracy 0.443, train loss 1734.8\n",
      "step 800, train accuracy 0.465, train loss 1657.39\n",
      "step 900, train accuracy 0.459, train loss 1613.5\n",
      "step 1000, train accuracy 0.481, train loss 1640.03\n",
      "step 1100, train accuracy 0.473, train loss 1636.3\n",
      "step 1200, train accuracy 0.484, train loss 1587.56\n",
      "step 1300, train accuracy 0.43, train loss 1616.25\n",
      "step 1400, train accuracy 0.494, train loss 1520.83\n",
      "step 1500, train accuracy 0.521, train loss 1513.54\n",
      "step 1600, train accuracy 0.513, train loss 1486.84\n",
      "step 1700, train accuracy 0.477, train loss 1506.23\n",
      "step 1800, train accuracy 0.495, train loss 1502.93\n",
      "step 1900, train accuracy 0.494, train loss 1434.48\n",
      "step 2000, train accuracy 0.504, train loss 1471.41\n",
      "step 2100, train accuracy 0.502, train loss 1490.97\n",
      "step 2200, train accuracy 0.51, train loss 1454.77\n",
      "step 2300, train accuracy 0.533, train loss 1427.63\n",
      "step 2400, train accuracy 0.534, train loss 1344.32\n",
      "step 2500, train accuracy 0.526, train loss 1371.84\n",
      "step 2600, train accuracy 0.523, train loss 1406.04\n",
      "step 2700, train accuracy 0.512, train loss 1389.88\n",
      "step 2800, train accuracy 0.551, train loss 1338.09\n",
      "step 2900, train accuracy 0.532, train loss 1333.51\n",
      "step 3000, train accuracy 0.572, train loss 1290.5\n",
      "step 3100, train accuracy 0.538, train loss 1342.77\n",
      "step 3200, train accuracy 0.534, train loss 1340.07\n",
      "step 3300, train accuracy 0.526, train loss 1312.7\n",
      "step 3400, train accuracy 0.524, train loss 1368.52\n",
      "step 3500, train accuracy 0.586, train loss 1261.22\n",
      "step 3600, train accuracy 0.54, train loss 1364.58\n",
      "step 3700, train accuracy 0.551, train loss 1287.25\n",
      "step 3800, train accuracy 0.54, train loss 1326.97\n",
      "step 3900, train accuracy 0.565, train loss 1273.79\n",
      "step 4000, train accuracy 0.591, train loss 1197.23\n",
      "step 4100, train accuracy 0.558, train loss 1283.84\n",
      "step 4200, train accuracy 0.565, train loss 1249.08\n",
      "step 4300, train accuracy 0.507, train loss 1437.68\n",
      "step 4400, train accuracy 0.567, train loss 1264.14\n",
      "step 4500, train accuracy 0.579, train loss 1220.98\n",
      "step 4600, train accuracy 0.578, train loss 1223.99\n",
      "step 4700, train accuracy 0.574, train loss 1232.73\n",
      "step 4800, train accuracy 0.544, train loss 1239.25\n",
      "step 4900, train accuracy 0.568, train loss 1245.7\n",
      "step 5000, train accuracy 0.554, train loss 1277.91\n",
      "step 5100, train accuracy 0.513, train loss 1376.98\n",
      "step 5200, train accuracy 0.598, train loss 1187.3\n",
      "step 5300, train accuracy 0.572, train loss 1231.41\n",
      "step 5400, train accuracy 0.554, train loss 1232.06\n",
      "step 5500, train accuracy 0.595, train loss 1142.21\n",
      "step 5600, train accuracy 0.582, train loss 1191.98\n",
      "step 5700, train accuracy 0.613, train loss 1181.66\n",
      "step 5800, train accuracy 0.588, train loss 1219.4\n",
      "step 5900, train accuracy 0.586, train loss 1155.57\n",
      "step 6000, train accuracy 0.605, train loss 1148.56\n",
      "step 6100, train accuracy 0.583, train loss 1187.59\n",
      "step 6200, train accuracy 0.599, train loss 1170.12\n",
      "step 6300, train accuracy 0.605, train loss 1131.09\n",
      "step 6400, train accuracy 0.597, train loss 1154.01\n",
      "step 6500, train accuracy 0.598, train loss 1187.72\n",
      "step 6600, train accuracy 0.581, train loss 1166.95\n",
      "step 6700, train accuracy 0.641, train loss 1061.72\n",
      "step 6800, train accuracy 0.586, train loss 1164.59\n",
      "step 6900, train accuracy 0.611, train loss 1116.56\n",
      "step 7000, train accuracy 0.612, train loss 1152.7\n",
      "step 7100, train accuracy 0.634, train loss 1052.74\n",
      "step 7200, train accuracy 0.624, train loss 1082.59\n",
      "step 7300, train accuracy 0.602, train loss 1140.39\n",
      "step 7400, train accuracy 0.599, train loss 1104.58\n",
      "step 7500, train accuracy 0.581, train loss 1146.7\n",
      "step 7600, train accuracy 0.59, train loss 1133.72\n",
      "step 7700, train accuracy 0.64, train loss 1075.41\n",
      "step 7800, train accuracy 0.636, train loss 1089.56\n",
      "step 7900, train accuracy 0.619, train loss 1105.74\n",
      "step 8000, train accuracy 0.609, train loss 1123.91\n",
      "step 8100, train accuracy 0.626, train loss 1054.63\n",
      "step 8200, train accuracy 0.642, train loss 1023.92\n",
      "step 8300, train accuracy 0.611, train loss 1072.17\n",
      "step 8400, train accuracy 0.623, train loss 1039.23\n",
      "step 8500, train accuracy 0.633, train loss 1083.31\n",
      "step 8600, train accuracy 0.633, train loss 1051.76\n",
      "step 8700, train accuracy 0.599, train loss 1125.22\n",
      "step 8800, train accuracy 0.582, train loss 1197.34\n",
      "step 8900, train accuracy 0.623, train loss 1067.32\n",
      "step 9000, train accuracy 0.664, train loss 1026.62\n",
      "step 9100, train accuracy 0.652, train loss 1039.59\n",
      "step 9200, train accuracy 0.617, train loss 1103.79\n",
      "step 9300, train accuracy 0.614, train loss 1084.83\n",
      "step 9400, train accuracy 0.619, train loss 1080.68\n",
      "step 9500, train accuracy 0.617, train loss 1069.05\n",
      "step 9600, train accuracy 0.632, train loss 1023.88\n",
      "step 9700, train accuracy 0.62, train loss 1080.4\n",
      "step 9800, train accuracy 0.649, train loss 1023.19\n",
      "step 9900, train accuracy 0.641, train loss 1054.14\n",
      "step 10000, train accuracy 0.601, train loss 1104.46\n",
      "step 10100, train accuracy 0.627, train loss 1095.06\n",
      "step 10200, train accuracy 0.626, train loss 1042.2\n",
      "step 10300, train accuracy 0.638, train loss 1007.61\n",
      "step 10400, train accuracy 0.651, train loss 1012.65\n",
      "step 10500, train accuracy 0.647, train loss 1038.71\n",
      "step 10600, train accuracy 0.665, train loss 966.717\n",
      "step 10700, train accuracy 0.667, train loss 969.81\n",
      "step 10800, train accuracy 0.657, train loss 1008.19\n",
      "step 10900, train accuracy 0.631, train loss 1002.39\n",
      "step 11000, train accuracy 0.606, train loss 1099.12\n",
      "step 11100, train accuracy 0.659, train loss 1031.05\n",
      "step 11200, train accuracy 0.633, train loss 1027.36\n",
      "step 11300, train accuracy 0.679, train loss 945.992\n",
      "step 11400, train accuracy 0.652, train loss 974.051\n",
      "step 11500, train accuracy 0.633, train loss 1060.73\n",
      "step 11600, train accuracy 0.644, train loss 994.406\n",
      "step 11700, train accuracy 0.602, train loss 1118.93\n",
      "step 11800, train accuracy 0.67, train loss 963.437\n",
      "step 11900, train accuracy 0.677, train loss 941.559\n",
      "step 12000, train accuracy 0.675, train loss 919.503\n",
      "step 12100, train accuracy 0.673, train loss 977.408\n",
      "step 12200, train accuracy 0.651, train loss 1001.13\n",
      "step 12300, train accuracy 0.653, train loss 1028.04\n",
      "step 12400, train accuracy 0.667, train loss 956.975\n",
      "step 12500, train accuracy 0.659, train loss 979.781\n",
      "step 12600, train accuracy 0.685, train loss 942.757\n",
      "step 12700, train accuracy 0.659, train loss 970.877\n",
      "step 12800, train accuracy 0.675, train loss 969.262\n",
      "step 12900, train accuracy 0.674, train loss 954.409\n",
      "step 13000, train accuracy 0.673, train loss 954.196\n",
      "step 13100, train accuracy 0.673, train loss 929.46\n",
      "step 13200, train accuracy 0.658, train loss 960.992\n",
      "step 13300, train accuracy 0.681, train loss 947.651\n",
      "step 13400, train accuracy 0.656, train loss 982.652\n",
      "step 13500, train accuracy 0.684, train loss 926.954\n",
      "step 13600, train accuracy 0.664, train loss 966.454\n",
      "step 13700, train accuracy 0.652, train loss 992.004\n",
      "step 13800, train accuracy 0.682, train loss 922.672\n",
      "step 13900, train accuracy 0.719, train loss 838.965\n",
      "step 14000, train accuracy 0.652, train loss 987.906\n",
      "step 14100, train accuracy 0.697, train loss 894.181\n",
      "step 14200, train accuracy 0.699, train loss 892.781\n",
      "step 14300, train accuracy 0.664, train loss 959.778\n",
      "step 14400, train accuracy 0.694, train loss 916.88\n",
      "step 14500, train accuracy 0.684, train loss 912.912\n",
      "step 14600, train accuracy 0.692, train loss 903.773\n",
      "step 14700, train accuracy 0.694, train loss 869.624\n",
      "step 14800, train accuracy 0.688, train loss 893.089\n",
      "step 14900, train accuracy 0.678, train loss 924.836\n",
      "step 15000, train accuracy 0.711, train loss 852.165\n",
      "step 15100, train accuracy 0.668, train loss 889.026\n",
      "step 15200, train accuracy 0.69, train loss 926.404\n",
      "step 15300, train accuracy 0.699, train loss 877.335\n",
      "step 15400, train accuracy 0.704, train loss 897.908\n",
      "step 15500, train accuracy 0.698, train loss 877.902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15600, train accuracy 0.676, train loss 954.325\n",
      "step 15700, train accuracy 0.677, train loss 938.683\n",
      "step 15800, train accuracy 0.665, train loss 945.227\n",
      "step 15900, train accuracy 0.668, train loss 956.958\n",
      "step 16000, train accuracy 0.683, train loss 936.14\n",
      "step 16100, train accuracy 0.684, train loss 898.429\n",
      "step 16200, train accuracy 0.704, train loss 857.726\n",
      "step 16300, train accuracy 0.687, train loss 850.602\n",
      "step 16400, train accuracy 0.697, train loss 891.982\n",
      "step 16500, train accuracy 0.686, train loss 905.109\n",
      "step 16600, train accuracy 0.708, train loss 866.976\n",
      "step 16700, train accuracy 0.697, train loss 891.759\n",
      "step 16800, train accuracy 0.704, train loss 877.802\n",
      "step 16900, train accuracy 0.702, train loss 866.434\n",
      "step 17000, train accuracy 0.686, train loss 887.57\n",
      "step 17100, train accuracy 0.694, train loss 894.628\n",
      "step 17200, train accuracy 0.689, train loss 875.747\n",
      "step 17300, train accuracy 0.711, train loss 866.391\n",
      "step 17400, train accuracy 0.709, train loss 861.347\n",
      "step 17500, train accuracy 0.707, train loss 853.784\n",
      "step 17600, train accuracy 0.684, train loss 909.031\n",
      "step 17700, train accuracy 0.701, train loss 878.203\n",
      "step 17800, train accuracy 0.723, train loss 838.318\n",
      "step 17900, train accuracy 0.671, train loss 924.208\n",
      "step 18000, train accuracy 0.699, train loss 886.093\n",
      "step 18100, train accuracy 0.716, train loss 809.733\n",
      "step 18200, train accuracy 0.713, train loss 887.57\n",
      "step 18300, train accuracy 0.7, train loss 855.633\n",
      "step 18400, train accuracy 0.709, train loss 847.599\n",
      "step 18500, train accuracy 0.688, train loss 867.907\n",
      "step 18600, train accuracy 0.705, train loss 865.616\n",
      "step 18700, train accuracy 0.669, train loss 917.936\n",
      "step 18800, train accuracy 0.697, train loss 852.609\n",
      "step 18900, train accuracy 0.71, train loss 852.704\n",
      "step 19000, train accuracy 0.673, train loss 891.861\n",
      "step 19100, train accuracy 0.693, train loss 878.941\n",
      "step 19200, train accuracy 0.712, train loss 842.812\n",
      "step 19300, train accuracy 0.699, train loss 845.668\n",
      "step 19400, train accuracy 0.697, train loss 869.895\n",
      "step 19500, train accuracy 0.709, train loss 830.797\n",
      "step 19600, train accuracy 0.706, train loss 871.032\n",
      "step 19700, train accuracy 0.712, train loss 829.603\n",
      "step 19800, train accuracy 0.708, train loss 876.955\n",
      "step 19900, train accuracy 0.736, train loss 778.572\n",
      "step 20000, train accuracy 0.728, train loss 828.466\n",
      "step 20100, train accuracy 0.737, train loss 795.355\n",
      "step 20200, train accuracy 0.7, train loss 847.254\n",
      "step 20300, train accuracy 0.67, train loss 945.591\n",
      "step 20400, train accuracy 0.724, train loss 814.653\n",
      "step 20500, train accuracy 0.71, train loss 860.837\n",
      "step 20600, train accuracy 0.757, train loss 740.116\n",
      "step 20700, train accuracy 0.729, train loss 782.63\n",
      "step 20800, train accuracy 0.742, train loss 753.2\n",
      "step 20900, train accuracy 0.705, train loss 849.265\n",
      "step 21000, train accuracy 0.724, train loss 797.362\n",
      "step 21100, train accuracy 0.736, train loss 752.937\n",
      "step 21200, train accuracy 0.706, train loss 828.891\n",
      "step 21300, train accuracy 0.706, train loss 822.091\n",
      "step 21400, train accuracy 0.708, train loss 848.204\n",
      "step 21500, train accuracy 0.738, train loss 786.266\n",
      "step 21600, train accuracy 0.745, train loss 750.939\n",
      "step 21700, train accuracy 0.68, train loss 928.731\n",
      "step 21800, train accuracy 0.698, train loss 837.317\n",
      "step 21900, train accuracy 0.689, train loss 856.191\n",
      "step 22000, train accuracy 0.71, train loss 847.897\n",
      "step 22100, train accuracy 0.721, train loss 793.145\n",
      "step 22200, train accuracy 0.716, train loss 822.144\n",
      "step 22300, train accuracy 0.7, train loss 829.498\n",
      "step 22400, train accuracy 0.706, train loss 829.675\n",
      "step 22500, train accuracy 0.729, train loss 779.5\n",
      "step 22600, train accuracy 0.739, train loss 748.198\n",
      "step 22700, train accuracy 0.728, train loss 767.474\n",
      "step 22800, train accuracy 0.695, train loss 884.015\n",
      "step 22900, train accuracy 0.708, train loss 806.135\n",
      "step 23000, train accuracy 0.735, train loss 778.774\n",
      "step 23100, train accuracy 0.729, train loss 779.207\n",
      "step 23200, train accuracy 0.696, train loss 860.207\n",
      "step 23300, train accuracy 0.715, train loss 816.762\n",
      "step 23400, train accuracy 0.735, train loss 748.722\n",
      "step 23500, train accuracy 0.736, train loss 767.478\n",
      "step 23600, train accuracy 0.733, train loss 794.698\n",
      "step 23700, train accuracy 0.721, train loss 791.985\n",
      "step 23800, train accuracy 0.743, train loss 749.96\n",
      "step 23900, train accuracy 0.747, train loss 741.04\n",
      "step 24000, train accuracy 0.71, train loss 816.766\n",
      "step 24100, train accuracy 0.729, train loss 765.03\n",
      "step 24200, train accuracy 0.72, train loss 800.851\n",
      "step 24300, train accuracy 0.723, train loss 767.264\n",
      "step 24400, train accuracy 0.736, train loss 775.05\n",
      "step 24500, train accuracy 0.733, train loss 763.714\n",
      "step 24600, train accuracy 0.733, train loss 736.5\n",
      "step 24700, train accuracy 0.707, train loss 882.348\n",
      "step 24800, train accuracy 0.734, train loss 738.075\n",
      "step 24900, train accuracy 0.736, train loss 749.608\n",
      "step 25000, train accuracy 0.72, train loss 808.122\n",
      "step 25100, train accuracy 0.756, train loss 723.559\n",
      "step 25200, train accuracy 0.731, train loss 755.464\n",
      "step 25300, train accuracy 0.734, train loss 760.839\n",
      "step 25400, train accuracy 0.738, train loss 753.741\n",
      "step 25500, train accuracy 0.739, train loss 740.181\n",
      "step 25600, train accuracy 0.721, train loss 799.863\n",
      "step 25700, train accuracy 0.715, train loss 819.561\n",
      "step 25800, train accuracy 0.741, train loss 775.789\n",
      "step 25900, train accuracy 0.728, train loss 751.774\n",
      "step 26000, train accuracy 0.747, train loss 732.977\n",
      "step 26100, train accuracy 0.706, train loss 855.201\n",
      "step 26200, train accuracy 0.735, train loss 769.693\n",
      "step 26300, train accuracy 0.736, train loss 755.354\n",
      "step 26400, train accuracy 0.739, train loss 732.643\n",
      "step 26500, train accuracy 0.76, train loss 747.2\n",
      "step 26600, train accuracy 0.736, train loss 710.282\n",
      "step 26700, train accuracy 0.751, train loss 708.247\n",
      "step 26800, train accuracy 0.754, train loss 710.7\n",
      "step 26900, train accuracy 0.716, train loss 802.041\n",
      "step 27000, train accuracy 0.69, train loss 894.223\n",
      "step 27100, train accuracy 0.714, train loss 785.249\n",
      "step 27200, train accuracy 0.721, train loss 809.984\n",
      "step 27300, train accuracy 0.739, train loss 738.742\n",
      "step 27400, train accuracy 0.719, train loss 762.526\n",
      "step 27500, train accuracy 0.724, train loss 808.891\n",
      "step 27600, train accuracy 0.763, train loss 725.093\n",
      "step 27700, train accuracy 0.768, train loss 701.611\n",
      "step 27800, train accuracy 0.753, train loss 714.686\n",
      "step 27900, train accuracy 0.748, train loss 728.817\n",
      "step 28000, train accuracy 0.759, train loss 707.311\n",
      "step 28100, train accuracy 0.728, train loss 777.319\n",
      "step 28200, train accuracy 0.729, train loss 790.44\n",
      "step 28300, train accuracy 0.77, train loss 719.566\n",
      "step 28400, train accuracy 0.772, train loss 679.368\n",
      "step 28500, train accuracy 0.773, train loss 679.809\n",
      "step 28600, train accuracy 0.718, train loss 829.355\n",
      "step 28700, train accuracy 0.746, train loss 756.508\n",
      "step 28800, train accuracy 0.754, train loss 714.943\n",
      "step 28900, train accuracy 0.736, train loss 736.566\n",
      "step 29000, train accuracy 0.775, train loss 662.079\n",
      "step 29100, train accuracy 0.755, train loss 727.47\n",
      "step 29200, train accuracy 0.737, train loss 790.794\n",
      "step 29300, train accuracy 0.771, train loss 692.176\n",
      "step 29400, train accuracy 0.749, train loss 724.519\n",
      "step 29500, train accuracy 0.744, train loss 730.331\n",
      "step 29600, train accuracy 0.733, train loss 766.156\n",
      "step 29700, train accuracy 0.768, train loss 718.603\n",
      "step 29800, train accuracy 0.747, train loss 726.296\n",
      "step 29900, train accuracy 0.751, train loss 700.092\n",
      "step 30000, train accuracy 0.76, train loss 705.966\n",
      "step 30100, train accuracy 0.748, train loss 739.74\n",
      "step 30200, train accuracy 0.734, train loss 759.634\n",
      "step 30300, train accuracy 0.749, train loss 753.82\n",
      "step 30400, train accuracy 0.753, train loss 720.386\n",
      "step 30500, train accuracy 0.736, train loss 771.808\n",
      "step 30600, train accuracy 0.779, train loss 693.262\n",
      "step 30700, train accuracy 0.739, train loss 728.429\n",
      "step 30800, train accuracy 0.738, train loss 721.585\n",
      "step 30900, train accuracy 0.76, train loss 714.207\n",
      "step 31000, train accuracy 0.722, train loss 780.686\n",
      "step 31100, train accuracy 0.764, train loss 699.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31200, train accuracy 0.753, train loss 707.119\n",
      "step 31300, train accuracy 0.752, train loss 718.478\n",
      "step 31400, train accuracy 0.755, train loss 711.653\n",
      "step 31500, train accuracy 0.757, train loss 679.311\n",
      "step 31600, train accuracy 0.754, train loss 685.469\n",
      "step 31700, train accuracy 0.765, train loss 675.99\n",
      "step 31800, train accuracy 0.779, train loss 662.433\n",
      "step 31900, train accuracy 0.769, train loss 686.487\n",
      "step 32000, train accuracy 0.775, train loss 662.424\n",
      "step 32100, train accuracy 0.735, train loss 774.924\n",
      "step 32200, train accuracy 0.781, train loss 652.11\n",
      "step 32300, train accuracy 0.765, train loss 698.79\n",
      "step 32400, train accuracy 0.753, train loss 723.291\n",
      "step 32500, train accuracy 0.773, train loss 658.731\n",
      "step 32600, train accuracy 0.774, train loss 677.959\n",
      "step 32700, train accuracy 0.755, train loss 761.534\n",
      "step 32800, train accuracy 0.788, train loss 638.574\n",
      "step 32900, train accuracy 0.759, train loss 675.5\n",
      "step 33000, train accuracy 0.771, train loss 659.007\n",
      "step 33100, train accuracy 0.779, train loss 687.003\n",
      "step 33200, train accuracy 0.756, train loss 714.527\n",
      "step 33300, train accuracy 0.749, train loss 720.573\n",
      "step 33400, train accuracy 0.803, train loss 643.439\n",
      "step 33500, train accuracy 0.722, train loss 773.061\n",
      "step 33600, train accuracy 0.732, train loss 728.714\n",
      "step 33700, train accuracy 0.788, train loss 636.507\n",
      "step 33800, train accuracy 0.768, train loss 678.72\n",
      "step 33900, train accuracy 0.76, train loss 695.028\n",
      "step 34000, train accuracy 0.793, train loss 617.877\n",
      "step 34100, train accuracy 0.778, train loss 628.089\n",
      "step 34200, train accuracy 0.779, train loss 651.057\n",
      "step 34300, train accuracy 0.78, train loss 699.504\n",
      "step 34400, train accuracy 0.757, train loss 708.09\n",
      "step 34500, train accuracy 0.762, train loss 663.051\n",
      "step 34600, train accuracy 0.787, train loss 648.503\n",
      "step 34700, train accuracy 0.772, train loss 667.57\n",
      "step 34800, train accuracy 0.782, train loss 672.502\n",
      "step 34900, train accuracy 0.761, train loss 671.412\n",
      "step 35000, train accuracy 0.778, train loss 671.671\n",
      "step 35100, train accuracy 0.792, train loss 665.574\n",
      "step 35200, train accuracy 0.757, train loss 678.636\n",
      "step 35300, train accuracy 0.774, train loss 681.521\n",
      "step 35400, train accuracy 0.761, train loss 665.298\n",
      "step 35500, train accuracy 0.779, train loss 642.333\n",
      "step 35600, train accuracy 0.789, train loss 625.673\n",
      "step 35700, train accuracy 0.778, train loss 650.173\n",
      "step 35800, train accuracy 0.76, train loss 704.489\n",
      "step 35900, train accuracy 0.781, train loss 660.276\n",
      "step 36000, train accuracy 0.792, train loss 613.218\n",
      "step 36100, train accuracy 0.792, train loss 621.66\n",
      "step 36200, train accuracy 0.745, train loss 709.32\n",
      "step 36300, train accuracy 0.778, train loss 656.322\n",
      "step 36400, train accuracy 0.748, train loss 705.772\n",
      "step 36500, train accuracy 0.762, train loss 684.771\n",
      "step 36600, train accuracy 0.72, train loss 736.475\n",
      "step 36700, train accuracy 0.798, train loss 606.501\n",
      "step 36800, train accuracy 0.773, train loss 638.389\n",
      "step 36900, train accuracy 0.811, train loss 593.687\n",
      "step 37000, train accuracy 0.754, train loss 678.731\n",
      "step 37100, train accuracy 0.758, train loss 668.454\n",
      "step 37200, train accuracy 0.786, train loss 666.2\n",
      "step 37300, train accuracy 0.777, train loss 658.588\n",
      "step 37400, train accuracy 0.796, train loss 638.495\n",
      "step 37500, train accuracy 0.787, train loss 619.35\n",
      "step 37600, train accuracy 0.718, train loss 768.049\n",
      "step 37700, train accuracy 0.754, train loss 687.613\n",
      "step 37800, train accuracy 0.746, train loss 689.357\n",
      "step 37900, train accuracy 0.769, train loss 652.919\n",
      "step 38000, train accuracy 0.793, train loss 612.748\n",
      "step 38100, train accuracy 0.767, train loss 642.314\n",
      "step 38200, train accuracy 0.776, train loss 650.961\n",
      "step 38300, train accuracy 0.799, train loss 631.075\n",
      "step 38400, train accuracy 0.773, train loss 635.018\n",
      "step 38500, train accuracy 0.784, train loss 634.789\n",
      "step 38600, train accuracy 0.766, train loss 651.11\n",
      "step 38700, train accuracy 0.765, train loss 656.733\n",
      "step 38800, train accuracy 0.785, train loss 657.334\n",
      "step 38900, train accuracy 0.781, train loss 639.118\n",
      "step 39000, train accuracy 0.783, train loss 639.344\n",
      "step 39100, train accuracy 0.758, train loss 705.882\n",
      "step 39200, train accuracy 0.765, train loss 661.744\n",
      "step 39300, train accuracy 0.785, train loss 628.894\n",
      "step 39400, train accuracy 0.783, train loss 614.961\n",
      "step 39500, train accuracy 0.756, train loss 707.775\n",
      "step 39600, train accuracy 0.774, train loss 666.112\n",
      "step 39700, train accuracy 0.782, train loss 607.321\n",
      "step 39800, train accuracy 0.785, train loss 629.722\n",
      "step 39900, train accuracy 0.797, train loss 598.726\n",
      "step 40000, train accuracy 0.782, train loss 643.269\n",
      "step 40100, train accuracy 0.809, train loss 582.024\n",
      "step 40200, train accuracy 0.801, train loss 592.261\n",
      "step 40300, train accuracy 0.781, train loss 652.577\n",
      "step 40400, train accuracy 0.826, train loss 542.635\n",
      "step 40500, train accuracy 0.77, train loss 648.622\n",
      "step 40600, train accuracy 0.772, train loss 622.976\n",
      "step 40700, train accuracy 0.8, train loss 607.972\n",
      "step 40800, train accuracy 0.796, train loss 603.618\n",
      "step 40900, train accuracy 0.774, train loss 654.833\n",
      "step 41000, train accuracy 0.816, train loss 579.145\n",
      "step 41100, train accuracy 0.77, train loss 670.567\n",
      "step 41200, train accuracy 0.783, train loss 647.565\n",
      "step 41300, train accuracy 0.78, train loss 639.209\n",
      "step 41400, train accuracy 0.76, train loss 665.948\n",
      "step 41500, train accuracy 0.796, train loss 628.516\n",
      "step 41600, train accuracy 0.8, train loss 589.632\n",
      "step 41700, train accuracy 0.783, train loss 612.057\n",
      "step 41800, train accuracy 0.788, train loss 596.062\n",
      "step 41900, train accuracy 0.797, train loss 603.556\n",
      "step 42000, train accuracy 0.795, train loss 601.45\n",
      "step 42100, train accuracy 0.805, train loss 612.709\n",
      "step 42200, train accuracy 0.792, train loss 595.413\n",
      "step 42300, train accuracy 0.791, train loss 611.229\n",
      "step 42400, train accuracy 0.784, train loss 655.001\n",
      "step 42500, train accuracy 0.786, train loss 599.786\n",
      "step 42600, train accuracy 0.794, train loss 583.359\n",
      "step 42700, train accuracy 0.783, train loss 625.146\n",
      "step 42800, train accuracy 0.786, train loss 622.978\n",
      "step 42900, train accuracy 0.784, train loss 604.48\n",
      "step 43000, train accuracy 0.806, train loss 575.964\n",
      "step 43100, train accuracy 0.788, train loss 602.765\n",
      "step 43200, train accuracy 0.799, train loss 564.095\n",
      "step 43300, train accuracy 0.817, train loss 545.269\n",
      "step 43400, train accuracy 0.808, train loss 565.869\n",
      "step 43500, train accuracy 0.787, train loss 612.087\n",
      "step 43600, train accuracy 0.801, train loss 548.439\n",
      "step 43700, train accuracy 0.78, train loss 602.861\n",
      "step 43800, train accuracy 0.799, train loss 571.616\n",
      "step 43900, train accuracy 0.776, train loss 630.569\n",
      "step 44000, train accuracy 0.784, train loss 654.442\n",
      "step 44100, train accuracy 0.778, train loss 662.912\n",
      "step 44200, train accuracy 0.826, train loss 574.936\n",
      "step 44300, train accuracy 0.78, train loss 613.702\n",
      "step 44400, train accuracy 0.813, train loss 572.571\n",
      "step 44500, train accuracy 0.777, train loss 633.89\n",
      "step 44600, train accuracy 0.807, train loss 571.476\n",
      "step 44700, train accuracy 0.792, train loss 571.852\n",
      "step 44800, train accuracy 0.785, train loss 636.027\n",
      "step 44900, train accuracy 0.818, train loss 541.474\n",
      "step 45000, train accuracy 0.786, train loss 652.724\n",
      "step 45100, train accuracy 0.801, train loss 544.992\n",
      "step 45200, train accuracy 0.808, train loss 586.335\n",
      "step 45300, train accuracy 0.795, train loss 598.128\n",
      "step 45400, train accuracy 0.787, train loss 630.172\n",
      "step 45500, train accuracy 0.8, train loss 586.472\n",
      "step 45600, train accuracy 0.769, train loss 654.8\n",
      "step 45700, train accuracy 0.808, train loss 583.24\n",
      "step 45800, train accuracy 0.806, train loss 578.88\n",
      "step 45900, train accuracy 0.808, train loss 566.588\n",
      "step 46000, train accuracy 0.816, train loss 557.713\n",
      "step 46100, train accuracy 0.802, train loss 575.629\n",
      "step 46200, train accuracy 0.816, train loss 538.364\n",
      "step 46300, train accuracy 0.813, train loss 578.971\n",
      "step 46400, train accuracy 0.799, train loss 573.596\n",
      "step 46500, train accuracy 0.804, train loss 588.529\n",
      "step 46600, train accuracy 0.775, train loss 614.98\n",
      "step 46700, train accuracy 0.822, train loss 513.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 46800, train accuracy 0.804, train loss 565.03\n",
      "step 46900, train accuracy 0.772, train loss 638.223\n",
      "step 47000, train accuracy 0.813, train loss 567.713\n",
      "step 47100, train accuracy 0.799, train loss 554.607\n",
      "step 47200, train accuracy 0.813, train loss 559.093\n",
      "step 47300, train accuracy 0.789, train loss 604.879\n",
      "step 47400, train accuracy 0.769, train loss 617.619\n",
      "step 47500, train accuracy 0.811, train loss 569.787\n",
      "step 47600, train accuracy 0.781, train loss 604.604\n",
      "step 47700, train accuracy 0.791, train loss 573.823\n",
      "step 47800, train accuracy 0.834, train loss 514.003\n",
      "step 47900, train accuracy 0.817, train loss 575.348\n",
      "step 48000, train accuracy 0.802, train loss 603.96\n",
      "step 48100, train accuracy 0.801, train loss 593.991\n",
      "step 48200, train accuracy 0.839, train loss 495.221\n",
      "step 48300, train accuracy 0.812, train loss 557.467\n",
      "step 48400, train accuracy 0.825, train loss 523.106\n",
      "step 48500, train accuracy 0.846, train loss 498.472\n",
      "step 48600, train accuracy 0.811, train loss 571.991\n",
      "step 48700, train accuracy 0.797, train loss 586.685\n",
      "step 48800, train accuracy 0.791, train loss 557.444\n",
      "step 48900, train accuracy 0.8, train loss 594.316\n",
      "step 49000, train accuracy 0.821, train loss 550.875\n",
      "step 49100, train accuracy 0.803, train loss 589.953\n",
      "step 49200, train accuracy 0.817, train loss 544.752\n",
      "step 49300, train accuracy 0.798, train loss 561.849\n",
      "step 49400, train accuracy 0.799, train loss 557.48\n",
      "step 49500, train accuracy 0.811, train loss 585.838\n",
      "step 49600, train accuracy 0.809, train loss 545.57\n",
      "step 49700, train accuracy 0.793, train loss 578.589\n",
      "step 49800, train accuracy 0.805, train loss 544.498\n",
      "step 49900, train accuracy 0.787, train loss 615.757\n",
      "step 50000, train accuracy 0.794, train loss 547.292\n",
      "step 50100, train accuracy 0.822, train loss 557.609\n",
      "step 50200, train accuracy 0.807, train loss 543.103\n",
      "step 50300, train accuracy 0.814, train loss 564.557\n",
      "step 50400, train accuracy 0.8, train loss 573.949\n",
      "step 50500, train accuracy 0.805, train loss 555.429\n",
      "step 50600, train accuracy 0.795, train loss 573.325\n",
      "step 50700, train accuracy 0.825, train loss 522.082\n",
      "step 50800, train accuracy 0.823, train loss 556.22\n",
      "step 50900, train accuracy 0.82, train loss 535.521\n",
      "step 51000, train accuracy 0.805, train loss 515.01\n",
      "step 51100, train accuracy 0.825, train loss 516.551\n",
      "step 51200, train accuracy 0.818, train loss 527.647\n",
      "step 51300, train accuracy 0.821, train loss 525.738\n",
      "step 51400, train accuracy 0.787, train loss 621.367\n",
      "step 51500, train accuracy 0.823, train loss 535.297\n",
      "step 51600, train accuracy 0.835, train loss 511.366\n",
      "step 51700, train accuracy 0.838, train loss 506.368\n",
      "step 51800, train accuracy 0.825, train loss 557.409\n",
      "step 51900, train accuracy 0.824, train loss 522.867\n",
      "step 52000, train accuracy 0.809, train loss 559.318\n",
      "step 52100, train accuracy 0.803, train loss 574.282\n",
      "step 52200, train accuracy 0.813, train loss 537.342\n",
      "step 52300, train accuracy 0.819, train loss 532.001\n",
      "step 52400, train accuracy 0.844, train loss 493.432\n",
      "step 52500, train accuracy 0.826, train loss 541.678\n",
      "step 52600, train accuracy 0.824, train loss 511.581\n",
      "step 52700, train accuracy 0.821, train loss 537.152\n",
      "step 52800, train accuracy 0.804, train loss 577.555\n",
      "step 52900, train accuracy 0.813, train loss 542.73\n",
      "step 53000, train accuracy 0.824, train loss 517.544\n",
      "step 53100, train accuracy 0.824, train loss 523.046\n",
      "step 53200, train accuracy 0.811, train loss 537.214\n",
      "step 53300, train accuracy 0.819, train loss 490.622\n",
      "step 53400, train accuracy 0.832, train loss 503.407\n",
      "step 53500, train accuracy 0.821, train loss 517.221\n",
      "step 53600, train accuracy 0.806, train loss 564.896\n",
      "step 53700, train accuracy 0.831, train loss 506.129\n",
      "step 53800, train accuracy 0.833, train loss 483.332\n",
      "step 53900, train accuracy 0.818, train loss 510.784\n",
      "step 54000, train accuracy 0.776, train loss 671.418\n",
      "step 54100, train accuracy 0.821, train loss 537.973\n",
      "step 54200, train accuracy 0.829, train loss 492.53\n",
      "step 54300, train accuracy 0.811, train loss 538.416\n",
      "step 54400, train accuracy 0.818, train loss 542.442\n",
      "step 54500, train accuracy 0.802, train loss 566.192\n",
      "step 54600, train accuracy 0.853, train loss 491.158\n",
      "step 54700, train accuracy 0.817, train loss 538.884\n",
      "step 54800, train accuracy 0.818, train loss 499.419\n",
      "step 54900, train accuracy 0.811, train loss 523.069\n",
      "step 55000, train accuracy 0.808, train loss 590.42\n",
      "step 55100, train accuracy 0.829, train loss 492.506\n",
      "step 55200, train accuracy 0.83, train loss 503.032\n",
      "step 55300, train accuracy 0.83, train loss 496.122\n",
      "step 55400, train accuracy 0.838, train loss 491.365\n",
      "step 55500, train accuracy 0.817, train loss 533.218\n",
      "step 55600, train accuracy 0.848, train loss 488.235\n",
      "step 55700, train accuracy 0.81, train loss 525.25\n",
      "step 55800, train accuracy 0.803, train loss 563.158\n",
      "step 55900, train accuracy 0.837, train loss 512.889\n",
      "step 56000, train accuracy 0.849, train loss 482.115\n",
      "step 56100, train accuracy 0.802, train loss 572.52\n",
      "step 56200, train accuracy 0.831, train loss 499.803\n",
      "step 56300, train accuracy 0.824, train loss 524.765\n",
      "step 56400, train accuracy 0.819, train loss 548.466\n",
      "step 56500, train accuracy 0.826, train loss 507.603\n",
      "step 56600, train accuracy 0.826, train loss 509.621\n",
      "step 56700, train accuracy 0.815, train loss 540.024\n",
      "step 56800, train accuracy 0.848, train loss 469.575\n",
      "step 56900, train accuracy 0.815, train loss 544.22\n",
      "step 57000, train accuracy 0.835, train loss 493.871\n",
      "step 57100, train accuracy 0.804, train loss 561.56\n",
      "step 57200, train accuracy 0.849, train loss 448.001\n",
      "step 57300, train accuracy 0.835, train loss 503.877\n",
      "step 57400, train accuracy 0.835, train loss 491.371\n",
      "step 57500, train accuracy 0.818, train loss 507.677\n",
      "step 57600, train accuracy 0.817, train loss 522.849\n",
      "step 57700, train accuracy 0.82, train loss 528.702\n",
      "step 57800, train accuracy 0.794, train loss 571.764\n",
      "step 57900, train accuracy 0.821, train loss 537.974\n",
      "step 58000, train accuracy 0.849, train loss 455.237\n",
      "step 58100, train accuracy 0.851, train loss 451.633\n",
      "step 58200, train accuracy 0.837, train loss 479.818\n",
      "step 58300, train accuracy 0.812, train loss 547.399\n",
      "step 58400, train accuracy 0.819, train loss 498.017\n",
      "step 58500, train accuracy 0.849, train loss 462.026\n",
      "step 58600, train accuracy 0.823, train loss 510.584\n",
      "step 58700, train accuracy 0.821, train loss 518.886\n",
      "step 58800, train accuracy 0.827, train loss 471.702\n",
      "step 58900, train accuracy 0.825, train loss 508.146\n",
      "step 59000, train accuracy 0.835, train loss 475.693\n",
      "step 59100, train accuracy 0.831, train loss 497.348\n",
      "step 59200, train accuracy 0.855, train loss 454.679\n",
      "step 59300, train accuracy 0.804, train loss 542.917\n",
      "step 59400, train accuracy 0.836, train loss 480.92\n",
      "step 59500, train accuracy 0.837, train loss 495.393\n",
      "step 59600, train accuracy 0.838, train loss 509.408\n",
      "step 59700, train accuracy 0.838, train loss 486.756\n",
      "step 59800, train accuracy 0.858, train loss 458.002\n",
      "step 59900, train accuracy 0.806, train loss 541.382\n",
      "step 60000, train accuracy 0.843, train loss 478.833\n",
      "step 60100, train accuracy 0.823, train loss 516.612\n",
      "step 60200, train accuracy 0.833, train loss 464.061\n",
      "step 60300, train accuracy 0.82, train loss 532.401\n",
      "step 60400, train accuracy 0.844, train loss 468.923\n",
      "step 60500, train accuracy 0.831, train loss 480.647\n",
      "step 60600, train accuracy 0.829, train loss 493.577\n",
      "step 60700, train accuracy 0.81, train loss 512.668\n",
      "step 60800, train accuracy 0.839, train loss 511.631\n",
      "step 60900, train accuracy 0.816, train loss 524.244\n",
      "step 61000, train accuracy 0.81, train loss 527.622\n",
      "step 61100, train accuracy 0.854, train loss 444.278\n",
      "step 61200, train accuracy 0.84, train loss 514.725\n",
      "step 61300, train accuracy 0.822, train loss 506.819\n",
      "step 61400, train accuracy 0.841, train loss 445.182\n",
      "step 61500, train accuracy 0.826, train loss 470.779\n",
      "step 61600, train accuracy 0.85, train loss 474.627\n",
      "step 61700, train accuracy 0.854, train loss 438.077\n",
      "step 61800, train accuracy 0.84, train loss 476.428\n",
      "step 61900, train accuracy 0.838, train loss 539.312\n",
      "step 62000, train accuracy 0.825, train loss 510.596\n",
      "step 62100, train accuracy 0.847, train loss 449.904\n",
      "step 62200, train accuracy 0.832, train loss 498.691\n",
      "step 62300, train accuracy 0.84, train loss 458.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62400, train accuracy 0.815, train loss 550.363\n",
      "step 62500, train accuracy 0.841, train loss 478.101\n",
      "step 62600, train accuracy 0.85, train loss 453.977\n",
      "step 62700, train accuracy 0.826, train loss 493.472\n",
      "step 62800, train accuracy 0.812, train loss 539.921\n",
      "step 62900, train accuracy 0.849, train loss 480.717\n",
      "step 63000, train accuracy 0.822, train loss 490.094\n",
      "step 63100, train accuracy 0.804, train loss 556.762\n",
      "step 63200, train accuracy 0.855, train loss 461.987\n",
      "step 63300, train accuracy 0.821, train loss 513.046\n",
      "step 63400, train accuracy 0.854, train loss 444.165\n",
      "step 63500, train accuracy 0.847, train loss 440.328\n",
      "step 63600, train accuracy 0.831, train loss 516.129\n",
      "step 63700, train accuracy 0.841, train loss 477.856\n",
      "step 63800, train accuracy 0.839, train loss 474.349\n",
      "step 63900, train accuracy 0.834, train loss 495.378\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession           F1 = 0.711                 \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data_shuffle.npy'\n",
    "vali_filename = 'E:/Alibaba German AI Challenge/data_process/sample_of_training.npy'\n",
    "data = np.load(filename)\n",
    "vali = np.load(vali_filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "print('The shape of vali is ',vali.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# flatten\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-5).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "Eval = []\n",
    "\n",
    "batch_size = 1000\n",
    "for i in range(64000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    temp_loss = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    vali_accuracy = accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "    Loss.append(temp_loss)\n",
    "    Eval.append(vali_accuracy)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %g\" %(i, train_accuracy, temp_loss))\n",
    "        if train_accuracy > 0.98:\n",
    "            break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAE/CAYAAAB8YAsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGYWziIhIM/P/AeNbZyt/pnDgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15e41c35d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Eval)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Eval)\n",
    "plt.title('Eval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAE/CAYAAABbzor+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJClnDHdJknLm/wOKjIvUKq2nggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c31ac06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.75      0.40        71\n",
      "          1       0.67      0.56      0.61       342\n",
      "          2       0.82      0.48      0.61       474\n",
      "          3       0.40      0.75      0.53       119\n",
      "          4       0.39      0.64      0.48       217\n",
      "          5       0.67      0.54      0.60       487\n",
      "          6       0.46      0.93      0.62        40\n",
      "          7       0.88      0.51      0.65       586\n",
      "          8       0.49      0.64      0.55       188\n",
      "          9       0.34      0.49      0.40       158\n",
      "         10       0.95      0.85      0.90       622\n",
      "         11       0.27      0.82      0.41       125\n",
      "         12       0.61      0.75      0.67       134\n",
      "         13       0.85      0.50      0.63       585\n",
      "         14       0.38      1.00      0.55        26\n",
      "         15       0.60      0.96      0.73        91\n",
      "         16       0.98      0.97      0.98       735\n",
      "\n",
      "avg / total       0.75      0.67      0.68      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = y_conv.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "train_val_y = np.argmax(vali[:,-17:],axis = 1)\n",
    "pred_y = np.argmax(pred, axis = 1)\n",
    "print (classification_report(train_val_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('sixth_64k_batch_shuffle_balance_50k_train_SGD.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_data : shuffle 50k\n",
    "* vali_data : 5k\n",
    "* batch_size : 500\n",
    "* Optimizer : SGD\n",
    "* learning_rate : 1e-4\n",
    "* accuracy_threshold : 0.98\n",
    "* step_limit : 300K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "The shape of vali is  (5000, 18449)\n",
      "step 0, train accuracy 0.072, train loss 6248.11\n",
      "step 100, train accuracy 0.236, train loss 1299.84\n",
      "step 200, train accuracy 0.078, train loss 1375.32\n",
      "step 300, train accuracy 0.19, train loss 1245.89\n",
      "step 400, train accuracy 0.296, train loss 1114\n",
      "step 500, train accuracy 0.278, train loss 1061.22\n",
      "step 600, train accuracy 0.344, train loss 1003.64\n",
      "step 700, train accuracy 0.38, train loss 989.333\n",
      "step 800, train accuracy 0.352, train loss 961.495\n",
      "step 900, train accuracy 0.374, train loss 914.666\n",
      "step 1000, train accuracy 0.362, train loss 945.71\n",
      "step 1100, train accuracy 0.412, train loss 875.967\n",
      "step 1200, train accuracy 0.402, train loss 847.266\n",
      "step 1300, train accuracy 0.408, train loss 876.163\n",
      "step 1400, train accuracy 0.466, train loss 845.665\n",
      "step 1500, train accuracy 0.45, train loss 823.215\n",
      "step 1600, train accuracy 0.428, train loss 906.647\n",
      "step 1700, train accuracy 0.404, train loss 849.885\n",
      "step 1800, train accuracy 0.43, train loss 828.496\n",
      "step 1900, train accuracy 0.42, train loss 811.387\n",
      "step 2000, train accuracy 0.436, train loss 822.011\n",
      "step 2100, train accuracy 0.462, train loss 772.265\n",
      "step 2200, train accuracy 0.43, train loss 843.298\n",
      "step 2300, train accuracy 0.45, train loss 760.173\n",
      "step 2400, train accuracy 0.47, train loss 763.296\n",
      "step 2500, train accuracy 0.47, train loss 756.598\n",
      "step 2600, train accuracy 0.492, train loss 731.882\n",
      "step 2700, train accuracy 0.464, train loss 754.759\n",
      "step 2800, train accuracy 0.496, train loss 709.239\n",
      "step 2900, train accuracy 0.472, train loss 751.454\n",
      "step 3000, train accuracy 0.522, train loss 723.096\n",
      "step 3100, train accuracy 0.484, train loss 701.521\n",
      "step 3200, train accuracy 0.48, train loss 701.443\n",
      "step 3300, train accuracy 0.474, train loss 745.33\n",
      "step 3400, train accuracy 0.536, train loss 710.081\n",
      "step 3500, train accuracy 0.556, train loss 651.931\n",
      "step 3600, train accuracy 0.524, train loss 676.338\n",
      "step 3700, train accuracy 0.566, train loss 612.475\n",
      "step 3800, train accuracy 0.538, train loss 660.106\n",
      "step 3900, train accuracy 0.546, train loss 613.828\n",
      "step 4000, train accuracy 0.506, train loss 670.139\n",
      "step 4100, train accuracy 0.534, train loss 645.256\n",
      "step 4200, train accuracy 0.54, train loss 665.811\n",
      "step 4300, train accuracy 0.562, train loss 638.758\n",
      "step 4400, train accuracy 0.606, train loss 586.823\n",
      "step 4500, train accuracy 0.586, train loss 591.795\n",
      "step 4600, train accuracy 0.594, train loss 571.026\n",
      "step 4700, train accuracy 0.558, train loss 603.712\n",
      "step 4800, train accuracy 0.58, train loss 586.893\n",
      "step 4900, train accuracy 0.578, train loss 594.858\n",
      "step 5000, train accuracy 0.586, train loss 592.977\n",
      "step 5100, train accuracy 0.596, train loss 585.759\n",
      "step 5200, train accuracy 0.61, train loss 577.224\n",
      "step 5300, train accuracy 0.628, train loss 544.5\n",
      "step 5400, train accuracy 0.58, train loss 615.46\n",
      "step 5500, train accuracy 0.6, train loss 567.274\n",
      "step 5600, train accuracy 0.596, train loss 598.237\n",
      "step 5700, train accuracy 0.646, train loss 574.793\n",
      "step 5800, train accuracy 0.658, train loss 538.357\n",
      "step 5900, train accuracy 0.616, train loss 537.798\n",
      "step 6000, train accuracy 0.654, train loss 504.803\n",
      "step 6100, train accuracy 0.648, train loss 496.383\n",
      "step 6200, train accuracy 0.638, train loss 503.737\n",
      "step 6300, train accuracy 0.62, train loss 544.526\n",
      "step 6400, train accuracy 0.582, train loss 567.641\n",
      "step 6500, train accuracy 0.656, train loss 526.824\n",
      "step 6600, train accuracy 0.65, train loss 510.448\n",
      "step 6700, train accuracy 0.616, train loss 529.617\n",
      "step 6800, train accuracy 0.676, train loss 482.296\n",
      "step 6900, train accuracy 0.628, train loss 518.52\n",
      "step 7000, train accuracy 0.636, train loss 514.375\n",
      "step 7100, train accuracy 0.644, train loss 522.708\n",
      "step 7200, train accuracy 0.66, train loss 496.983\n",
      "step 7300, train accuracy 0.67, train loss 480.448\n",
      "step 7400, train accuracy 0.69, train loss 485.43\n",
      "step 7500, train accuracy 0.626, train loss 522.98\n",
      "step 7600, train accuracy 0.654, train loss 514.864\n",
      "step 7700, train accuracy 0.634, train loss 515.471\n",
      "step 7800, train accuracy 0.674, train loss 487.718\n",
      "step 7900, train accuracy 0.636, train loss 496.916\n",
      "step 8000, train accuracy 0.658, train loss 484.016\n",
      "step 8100, train accuracy 0.682, train loss 479.843\n",
      "step 8200, train accuracy 0.648, train loss 525.819\n",
      "step 8300, train accuracy 0.656, train loss 492.738\n",
      "step 8400, train accuracy 0.678, train loss 481.797\n",
      "step 8500, train accuracy 0.656, train loss 475.249\n",
      "step 8600, train accuracy 0.67, train loss 481.313\n",
      "step 8700, train accuracy 0.698, train loss 469.705\n",
      "step 8800, train accuracy 0.638, train loss 505.34\n",
      "step 8900, train accuracy 0.654, train loss 477.722\n",
      "step 9000, train accuracy 0.712, train loss 477.339\n",
      "step 9100, train accuracy 0.688, train loss 455.677\n",
      "step 9200, train accuracy 0.67, train loss 495.094\n",
      "step 9300, train accuracy 0.672, train loss 492.523\n",
      "step 9400, train accuracy 0.684, train loss 446.855\n",
      "step 9500, train accuracy 0.706, train loss 439.494\n",
      "step 9600, train accuracy 0.65, train loss 499.664\n",
      "step 9700, train accuracy 0.668, train loss 447.985\n",
      "step 9800, train accuracy 0.64, train loss 475.481\n",
      "step 9900, train accuracy 0.656, train loss 464.707\n",
      "step 10000, train accuracy 0.71, train loss 437.626\n",
      "step 10100, train accuracy 0.65, train loss 485.394\n",
      "step 10200, train accuracy 0.694, train loss 465.963\n",
      "step 10300, train accuracy 0.656, train loss 467.569\n",
      "step 10400, train accuracy 0.694, train loss 435.159\n",
      "step 10500, train accuracy 0.716, train loss 414.364\n",
      "step 10600, train accuracy 0.692, train loss 432.358\n",
      "step 10700, train accuracy 0.652, train loss 460.317\n",
      "step 10800, train accuracy 0.67, train loss 457.789\n",
      "step 10900, train accuracy 0.694, train loss 433.061\n",
      "step 11000, train accuracy 0.686, train loss 456.917\n",
      "step 11100, train accuracy 0.722, train loss 412.589\n",
      "step 11200, train accuracy 0.708, train loss 414.044\n",
      "step 11300, train accuracy 0.748, train loss 398.598\n",
      "step 11400, train accuracy 0.702, train loss 442.909\n",
      "step 11500, train accuracy 0.68, train loss 460.415\n",
      "step 11600, train accuracy 0.708, train loss 448.905\n",
      "step 11700, train accuracy 0.654, train loss 468.697\n",
      "step 11800, train accuracy 0.728, train loss 401.108\n",
      "step 11900, train accuracy 0.722, train loss 383.328\n",
      "step 12000, train accuracy 0.716, train loss 412.354\n",
      "step 12100, train accuracy 0.708, train loss 444.364\n",
      "step 12200, train accuracy 0.636, train loss 498.972\n",
      "step 12300, train accuracy 0.692, train loss 435.694\n",
      "step 12400, train accuracy 0.694, train loss 425.7\n",
      "step 12500, train accuracy 0.728, train loss 402.565\n",
      "step 12600, train accuracy 0.754, train loss 401.419\n",
      "step 12700, train accuracy 0.732, train loss 407.825\n",
      "step 12800, train accuracy 0.71, train loss 399.053\n",
      "step 12900, train accuracy 0.67, train loss 455.315\n",
      "step 13000, train accuracy 0.708, train loss 423.55\n",
      "step 13100, train accuracy 0.734, train loss 385.424\n",
      "step 13200, train accuracy 0.716, train loss 429.094\n",
      "step 13300, train accuracy 0.69, train loss 425.393\n",
      "step 13400, train accuracy 0.732, train loss 409.041\n",
      "step 13500, train accuracy 0.702, train loss 434.449\n",
      "step 13600, train accuracy 0.758, train loss 397.203\n",
      "step 13700, train accuracy 0.7, train loss 439.016\n",
      "step 13800, train accuracy 0.738, train loss 397.67\n",
      "step 13900, train accuracy 0.714, train loss 406.306\n",
      "step 14000, train accuracy 0.714, train loss 410.794\n",
      "step 14100, train accuracy 0.722, train loss 404.812\n",
      "step 14200, train accuracy 0.77, train loss 349.483\n",
      "step 14300, train accuracy 0.728, train loss 401.605\n",
      "step 14400, train accuracy 0.732, train loss 371.531\n",
      "step 14500, train accuracy 0.714, train loss 412.428\n",
      "step 14600, train accuracy 0.696, train loss 414.786\n",
      "step 14700, train accuracy 0.758, train loss 364.845\n",
      "step 14800, train accuracy 0.744, train loss 372.548\n",
      "step 14900, train accuracy 0.746, train loss 383.779\n",
      "step 15000, train accuracy 0.716, train loss 436.377\n",
      "step 15100, train accuracy 0.728, train loss 399.326\n",
      "step 15200, train accuracy 0.744, train loss 388.944\n",
      "step 15300, train accuracy 0.694, train loss 409.161\n",
      "step 15400, train accuracy 0.718, train loss 398.935\n",
      "step 15500, train accuracy 0.736, train loss 374.866\n",
      "step 15600, train accuracy 0.766, train loss 359.681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15700, train accuracy 0.734, train loss 375.553\n",
      "step 15800, train accuracy 0.766, train loss 354.943\n",
      "step 15900, train accuracy 0.726, train loss 389.61\n",
      "step 16000, train accuracy 0.764, train loss 384.788\n",
      "step 16100, train accuracy 0.7, train loss 424.65\n",
      "step 16200, train accuracy 0.702, train loss 420.181\n",
      "step 16300, train accuracy 0.746, train loss 356.219\n",
      "step 16400, train accuracy 0.754, train loss 368.572\n",
      "step 16500, train accuracy 0.734, train loss 370.722\n",
      "step 16600, train accuracy 0.746, train loss 364.51\n",
      "step 16700, train accuracy 0.724, train loss 416.568\n",
      "step 16800, train accuracy 0.778, train loss 350.549\n",
      "step 16900, train accuracy 0.734, train loss 392.765\n",
      "step 17000, train accuracy 0.752, train loss 388.88\n",
      "step 17100, train accuracy 0.71, train loss 400.108\n",
      "step 17200, train accuracy 0.702, train loss 416.091\n",
      "step 17300, train accuracy 0.74, train loss 371.035\n",
      "step 17400, train accuracy 0.756, train loss 362.604\n",
      "step 17500, train accuracy 0.744, train loss 351.28\n",
      "step 17600, train accuracy 0.764, train loss 336.025\n",
      "step 17700, train accuracy 0.762, train loss 351.705\n",
      "step 17800, train accuracy 0.8, train loss 335.885\n",
      "step 17900, train accuracy 0.736, train loss 357.01\n",
      "step 18000, train accuracy 0.748, train loss 369.81\n",
      "step 18100, train accuracy 0.782, train loss 314.462\n",
      "step 18200, train accuracy 0.79, train loss 324.354\n",
      "step 18300, train accuracy 0.73, train loss 366.63\n",
      "step 18400, train accuracy 0.742, train loss 398.59\n",
      "step 18500, train accuracy 0.752, train loss 366.302\n",
      "step 18600, train accuracy 0.756, train loss 347.302\n",
      "step 18700, train accuracy 0.794, train loss 321.457\n",
      "step 18800, train accuracy 0.738, train loss 377.067\n",
      "step 18900, train accuracy 0.754, train loss 340.658\n",
      "step 19000, train accuracy 0.778, train loss 315.081\n",
      "step 19100, train accuracy 0.806, train loss 311.867\n",
      "step 19200, train accuracy 0.774, train loss 329.76\n",
      "step 19300, train accuracy 0.748, train loss 333.649\n",
      "step 19400, train accuracy 0.782, train loss 313.55\n",
      "step 19500, train accuracy 0.776, train loss 344.574\n",
      "step 19600, train accuracy 0.748, train loss 381.373\n",
      "step 19700, train accuracy 0.764, train loss 345.809\n",
      "step 19800, train accuracy 0.774, train loss 328.44\n",
      "step 19900, train accuracy 0.768, train loss 326.101\n",
      "step 20000, train accuracy 0.774, train loss 338.553\n",
      "step 20100, train accuracy 0.772, train loss 340.042\n",
      "step 20200, train accuracy 0.738, train loss 371.192\n",
      "step 20300, train accuracy 0.744, train loss 373.556\n",
      "step 20400, train accuracy 0.756, train loss 326.486\n",
      "step 20500, train accuracy 0.768, train loss 316.081\n",
      "step 20600, train accuracy 0.756, train loss 370.596\n",
      "step 20700, train accuracy 0.768, train loss 343.618\n",
      "step 20800, train accuracy 0.824, train loss 282.189\n",
      "step 20900, train accuracy 0.75, train loss 348.537\n",
      "step 21000, train accuracy 0.77, train loss 349.097\n",
      "step 21100, train accuracy 0.752, train loss 349.259\n",
      "step 21200, train accuracy 0.792, train loss 291.39\n",
      "step 21300, train accuracy 0.748, train loss 349.509\n",
      "step 21400, train accuracy 0.734, train loss 342.851\n",
      "step 21500, train accuracy 0.764, train loss 310.736\n",
      "step 21600, train accuracy 0.762, train loss 364.779\n",
      "step 21700, train accuracy 0.808, train loss 307.857\n",
      "step 21800, train accuracy 0.74, train loss 373.092\n",
      "step 21900, train accuracy 0.754, train loss 325.946\n",
      "step 22000, train accuracy 0.762, train loss 330.948\n",
      "step 22100, train accuracy 0.792, train loss 311.842\n",
      "step 22200, train accuracy 0.812, train loss 285.986\n",
      "step 22300, train accuracy 0.758, train loss 357.009\n",
      "step 22400, train accuracy 0.722, train loss 377.044\n",
      "step 22500, train accuracy 0.714, train loss 388.453\n",
      "step 22600, train accuracy 0.762, train loss 347.948\n",
      "step 22700, train accuracy 0.766, train loss 316.238\n",
      "step 22800, train accuracy 0.758, train loss 351.36\n",
      "step 22900, train accuracy 0.784, train loss 335.449\n",
      "step 23000, train accuracy 0.768, train loss 338.863\n",
      "step 23100, train accuracy 0.78, train loss 325.274\n",
      "step 23200, train accuracy 0.732, train loss 365.381\n",
      "step 23300, train accuracy 0.8, train loss 310.703\n",
      "step 23400, train accuracy 0.758, train loss 358.628\n",
      "step 23500, train accuracy 0.806, train loss 308.3\n",
      "step 23600, train accuracy 0.794, train loss 297.537\n",
      "step 23700, train accuracy 0.744, train loss 352.702\n",
      "step 23800, train accuracy 0.816, train loss 280.552\n",
      "step 23900, train accuracy 0.794, train loss 300.943\n",
      "step 24000, train accuracy 0.77, train loss 320.312\n",
      "step 24100, train accuracy 0.79, train loss 326.084\n",
      "step 24200, train accuracy 0.776, train loss 333.51\n",
      "step 24300, train accuracy 0.806, train loss 284.959\n",
      "step 24400, train accuracy 0.784, train loss 325.528\n",
      "step 24500, train accuracy 0.782, train loss 326.958\n",
      "step 24600, train accuracy 0.762, train loss 339.154\n",
      "step 24700, train accuracy 0.758, train loss 361.545\n",
      "step 24800, train accuracy 0.792, train loss 312.278\n",
      "step 24900, train accuracy 0.77, train loss 342.059\n",
      "step 25000, train accuracy 0.778, train loss 302.91\n",
      "step 25100, train accuracy 0.792, train loss 285.681\n",
      "step 25200, train accuracy 0.76, train loss 319.878\n",
      "step 25300, train accuracy 0.786, train loss 322.716\n",
      "step 25400, train accuracy 0.802, train loss 288.388\n",
      "step 25500, train accuracy 0.812, train loss 280.172\n",
      "step 25600, train accuracy 0.826, train loss 267.506\n",
      "step 25700, train accuracy 0.798, train loss 297.647\n",
      "step 25800, train accuracy 0.804, train loss 279.884\n",
      "step 25900, train accuracy 0.762, train loss 327.609\n",
      "step 26000, train accuracy 0.796, train loss 325.278\n",
      "step 26100, train accuracy 0.794, train loss 301.331\n",
      "step 26200, train accuracy 0.772, train loss 342.358\n",
      "step 26300, train accuracy 0.756, train loss 319.041\n",
      "step 26400, train accuracy 0.814, train loss 271.332\n",
      "step 26500, train accuracy 0.808, train loss 273.42\n",
      "step 26600, train accuracy 0.796, train loss 313.707\n",
      "step 26700, train accuracy 0.774, train loss 309.63\n",
      "step 26800, train accuracy 0.816, train loss 267.899\n",
      "step 26900, train accuracy 0.784, train loss 296.52\n",
      "step 27000, train accuracy 0.812, train loss 282.926\n",
      "step 27100, train accuracy 0.788, train loss 328.182\n",
      "step 27200, train accuracy 0.802, train loss 260.97\n",
      "step 27300, train accuracy 0.776, train loss 296.672\n",
      "step 27400, train accuracy 0.808, train loss 276.305\n",
      "step 27500, train accuracy 0.786, train loss 309.257\n",
      "step 27600, train accuracy 0.802, train loss 266.509\n",
      "step 27700, train accuracy 0.788, train loss 300.819\n",
      "step 27800, train accuracy 0.794, train loss 269.781\n",
      "step 27900, train accuracy 0.764, train loss 304.975\n",
      "step 28000, train accuracy 0.796, train loss 288.128\n",
      "step 28100, train accuracy 0.802, train loss 313.025\n",
      "step 28200, train accuracy 0.8, train loss 271.864\n",
      "step 28300, train accuracy 0.766, train loss 303.819\n",
      "step 28400, train accuracy 0.79, train loss 300.951\n",
      "step 28500, train accuracy 0.78, train loss 293.027\n",
      "step 28600, train accuracy 0.784, train loss 282.43\n",
      "step 28700, train accuracy 0.806, train loss 280.7\n",
      "step 28800, train accuracy 0.84, train loss 246.315\n",
      "step 28900, train accuracy 0.79, train loss 279.759\n",
      "step 29000, train accuracy 0.836, train loss 242.008\n",
      "step 29100, train accuracy 0.774, train loss 312.512\n",
      "step 29200, train accuracy 0.818, train loss 276.913\n",
      "step 29300, train accuracy 0.806, train loss 278.381\n",
      "step 29400, train accuracy 0.778, train loss 308.819\n",
      "step 29500, train accuracy 0.79, train loss 303.232\n",
      "step 29600, train accuracy 0.762, train loss 308.343\n",
      "step 29700, train accuracy 0.782, train loss 297.05\n",
      "step 29800, train accuracy 0.806, train loss 281.039\n",
      "step 29900, train accuracy 0.826, train loss 282.308\n",
      "step 30000, train accuracy 0.806, train loss 275.829\n",
      "step 30100, train accuracy 0.81, train loss 278.968\n",
      "step 30200, train accuracy 0.776, train loss 275.642\n",
      "step 30300, train accuracy 0.816, train loss 304.591\n",
      "step 30400, train accuracy 0.778, train loss 301.628\n",
      "step 30500, train accuracy 0.84, train loss 277.989\n",
      "step 30600, train accuracy 0.814, train loss 286.38\n",
      "step 30700, train accuracy 0.8, train loss 280.406\n",
      "step 30800, train accuracy 0.8, train loss 308.223\n",
      "step 30900, train accuracy 0.816, train loss 246.796\n",
      "step 31000, train accuracy 0.8, train loss 297.619\n",
      "step 31100, train accuracy 0.824, train loss 268.384\n",
      "step 31200, train accuracy 0.8, train loss 288.391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31300, train accuracy 0.806, train loss 262.934\n",
      "step 31400, train accuracy 0.826, train loss 249.663\n",
      "step 31500, train accuracy 0.798, train loss 289.721\n",
      "step 31600, train accuracy 0.822, train loss 284.849\n",
      "step 31700, train accuracy 0.822, train loss 252.327\n",
      "step 31800, train accuracy 0.798, train loss 275.025\n",
      "step 31900, train accuracy 0.814, train loss 276.92\n",
      "step 32000, train accuracy 0.792, train loss 264.149\n",
      "step 32100, train accuracy 0.786, train loss 272.149\n",
      "step 32200, train accuracy 0.814, train loss 241.094\n",
      "step 32300, train accuracy 0.826, train loss 241.294\n",
      "step 32400, train accuracy 0.8, train loss 296.315\n",
      "step 32500, train accuracy 0.832, train loss 245.898\n",
      "step 32600, train accuracy 0.818, train loss 264.656\n",
      "step 32700, train accuracy 0.792, train loss 294.787\n",
      "step 32800, train accuracy 0.818, train loss 270.886\n",
      "step 32900, train accuracy 0.802, train loss 287.584\n",
      "step 33000, train accuracy 0.81, train loss 265.192\n",
      "step 33100, train accuracy 0.836, train loss 236.979\n",
      "step 33200, train accuracy 0.792, train loss 274.135\n",
      "step 33300, train accuracy 0.818, train loss 256.956\n",
      "step 33400, train accuracy 0.826, train loss 266.955\n",
      "step 33500, train accuracy 0.808, train loss 271.058\n",
      "step 33600, train accuracy 0.81, train loss 267.804\n",
      "step 33700, train accuracy 0.812, train loss 273.428\n",
      "step 33800, train accuracy 0.826, train loss 257.042\n",
      "step 33900, train accuracy 0.82, train loss 265.091\n",
      "step 34000, train accuracy 0.824, train loss 248.146\n",
      "step 34100, train accuracy 0.832, train loss 238.913\n",
      "step 34200, train accuracy 0.806, train loss 253.207\n",
      "step 34300, train accuracy 0.804, train loss 260.208\n",
      "step 34400, train accuracy 0.822, train loss 270.328\n",
      "step 34500, train accuracy 0.826, train loss 266.405\n",
      "step 34600, train accuracy 0.832, train loss 250.04\n",
      "step 34700, train accuracy 0.822, train loss 255.103\n",
      "step 34800, train accuracy 0.826, train loss 260.156\n",
      "step 34900, train accuracy 0.8, train loss 267.995\n",
      "step 35000, train accuracy 0.846, train loss 237.287\n",
      "step 35100, train accuracy 0.86, train loss 219.461\n",
      "step 35200, train accuracy 0.808, train loss 267.878\n",
      "step 35300, train accuracy 0.832, train loss 249.97\n",
      "step 35400, train accuracy 0.834, train loss 238.99\n",
      "step 35500, train accuracy 0.828, train loss 241.111\n",
      "step 35600, train accuracy 0.822, train loss 254.747\n",
      "step 35700, train accuracy 0.824, train loss 236.234\n",
      "step 35800, train accuracy 0.818, train loss 285.081\n",
      "step 35900, train accuracy 0.854, train loss 225.077\n",
      "step 36000, train accuracy 0.848, train loss 246.26\n",
      "step 36100, train accuracy 0.832, train loss 237.985\n",
      "step 36200, train accuracy 0.82, train loss 268.036\n",
      "step 36300, train accuracy 0.844, train loss 229.319\n",
      "step 36400, train accuracy 0.858, train loss 212.547\n",
      "step 36500, train accuracy 0.828, train loss 245.209\n",
      "step 36600, train accuracy 0.846, train loss 245.17\n",
      "step 36700, train accuracy 0.834, train loss 235.505\n",
      "step 36800, train accuracy 0.812, train loss 267.97\n",
      "step 36900, train accuracy 0.832, train loss 263.788\n",
      "step 37000, train accuracy 0.8, train loss 261.344\n",
      "step 37100, train accuracy 0.826, train loss 270.325\n",
      "step 37200, train accuracy 0.828, train loss 246.251\n",
      "step 37300, train accuracy 0.83, train loss 244.338\n",
      "step 37400, train accuracy 0.828, train loss 261.813\n",
      "step 37500, train accuracy 0.838, train loss 224.83\n",
      "step 37600, train accuracy 0.818, train loss 235.027\n",
      "step 37700, train accuracy 0.83, train loss 245.03\n",
      "step 37800, train accuracy 0.814, train loss 273.63\n",
      "step 37900, train accuracy 0.84, train loss 232.793\n",
      "step 38000, train accuracy 0.858, train loss 212.203\n",
      "step 38100, train accuracy 0.848, train loss 226.479\n",
      "step 38200, train accuracy 0.862, train loss 224.905\n",
      "step 38300, train accuracy 0.832, train loss 248.79\n",
      "step 38400, train accuracy 0.846, train loss 230.141\n",
      "step 38500, train accuracy 0.84, train loss 242.797\n",
      "step 38600, train accuracy 0.812, train loss 262.859\n",
      "step 38700, train accuracy 0.848, train loss 236.454\n",
      "step 38800, train accuracy 0.84, train loss 237.426\n",
      "step 38900, train accuracy 0.856, train loss 218.914\n",
      "step 39000, train accuracy 0.824, train loss 240.935\n",
      "step 39100, train accuracy 0.842, train loss 235.449\n",
      "step 39200, train accuracy 0.84, train loss 223.14\n",
      "step 39300, train accuracy 0.854, train loss 212.661\n",
      "step 39400, train accuracy 0.862, train loss 211.247\n",
      "step 39500, train accuracy 0.82, train loss 258.322\n",
      "step 39600, train accuracy 0.812, train loss 247.929\n",
      "step 39700, train accuracy 0.848, train loss 227.155\n",
      "step 39800, train accuracy 0.872, train loss 188.751\n",
      "step 39900, train accuracy 0.834, train loss 259.397\n",
      "step 40000, train accuracy 0.862, train loss 239.777\n",
      "step 40100, train accuracy 0.834, train loss 237.738\n",
      "step 40200, train accuracy 0.784, train loss 300.865\n",
      "step 40300, train accuracy 0.856, train loss 228.42\n",
      "step 40400, train accuracy 0.838, train loss 223.564\n",
      "step 40500, train accuracy 0.868, train loss 202.646\n",
      "step 40600, train accuracy 0.852, train loss 214.003\n",
      "step 40700, train accuracy 0.85, train loss 207.599\n",
      "step 40800, train accuracy 0.808, train loss 253.156\n",
      "step 40900, train accuracy 0.82, train loss 250.554\n",
      "step 41000, train accuracy 0.832, train loss 226.329\n",
      "step 41100, train accuracy 0.828, train loss 242.281\n",
      "step 41200, train accuracy 0.844, train loss 215.749\n",
      "step 41300, train accuracy 0.808, train loss 263.474\n",
      "step 41400, train accuracy 0.86, train loss 216.539\n",
      "step 41500, train accuracy 0.862, train loss 230.888\n",
      "step 41600, train accuracy 0.858, train loss 200.812\n",
      "step 41700, train accuracy 0.878, train loss 211.083\n",
      "step 41800, train accuracy 0.848, train loss 221.384\n",
      "step 41900, train accuracy 0.808, train loss 291.56\n",
      "step 42000, train accuracy 0.84, train loss 219.016\n",
      "step 42100, train accuracy 0.842, train loss 207.384\n",
      "step 42200, train accuracy 0.862, train loss 203.83\n",
      "step 42300, train accuracy 0.856, train loss 222.832\n",
      "step 42400, train accuracy 0.85, train loss 234.97\n",
      "step 42500, train accuracy 0.88, train loss 213.792\n",
      "step 42600, train accuracy 0.876, train loss 193.98\n",
      "step 42700, train accuracy 0.854, train loss 228.166\n",
      "step 42800, train accuracy 0.858, train loss 191.08\n",
      "step 42900, train accuracy 0.892, train loss 179.043\n",
      "step 43000, train accuracy 0.864, train loss 207.766\n",
      "step 43100, train accuracy 0.816, train loss 242.893\n",
      "step 43200, train accuracy 0.85, train loss 218.962\n",
      "step 43300, train accuracy 0.852, train loss 219.36\n",
      "step 43400, train accuracy 0.822, train loss 231.147\n",
      "step 43500, train accuracy 0.828, train loss 242.194\n",
      "step 43600, train accuracy 0.866, train loss 198.093\n",
      "step 43700, train accuracy 0.85, train loss 225.651\n",
      "step 43800, train accuracy 0.832, train loss 238.083\n",
      "step 43900, train accuracy 0.848, train loss 224.929\n",
      "step 44000, train accuracy 0.838, train loss 231.86\n",
      "step 44100, train accuracy 0.83, train loss 248.114\n",
      "step 44200, train accuracy 0.892, train loss 184.554\n",
      "step 44300, train accuracy 0.868, train loss 201.518\n",
      "step 44400, train accuracy 0.858, train loss 208.394\n",
      "step 44500, train accuracy 0.854, train loss 221.847\n",
      "step 44600, train accuracy 0.848, train loss 234.948\n",
      "step 44700, train accuracy 0.848, train loss 219.455\n",
      "step 44800, train accuracy 0.856, train loss 223.898\n",
      "step 44900, train accuracy 0.848, train loss 211.527\n",
      "step 45000, train accuracy 0.858, train loss 217.712\n",
      "step 45100, train accuracy 0.872, train loss 189.682\n",
      "step 45200, train accuracy 0.884, train loss 192.517\n",
      "step 45300, train accuracy 0.834, train loss 225.397\n",
      "step 45400, train accuracy 0.85, train loss 229.06\n",
      "step 45500, train accuracy 0.85, train loss 194.654\n",
      "step 45600, train accuracy 0.846, train loss 213.902\n",
      "step 45700, train accuracy 0.854, train loss 217.033\n",
      "step 45800, train accuracy 0.882, train loss 169.784\n",
      "step 45900, train accuracy 0.85, train loss 218.352\n",
      "step 46000, train accuracy 0.878, train loss 198.965\n",
      "step 46100, train accuracy 0.864, train loss 196.431\n",
      "step 46200, train accuracy 0.826, train loss 230.434\n",
      "step 46300, train accuracy 0.87, train loss 182.755\n",
      "step 46400, train accuracy 0.84, train loss 228.189\n",
      "step 46500, train accuracy 0.876, train loss 190.509\n",
      "step 46600, train accuracy 0.852, train loss 222.573\n",
      "step 46700, train accuracy 0.86, train loss 217.971\n",
      "step 46800, train accuracy 0.854, train loss 205.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 46900, train accuracy 0.898, train loss 179.957\n",
      "step 47000, train accuracy 0.852, train loss 217.257\n",
      "step 47100, train accuracy 0.868, train loss 199.965\n",
      "step 47200, train accuracy 0.852, train loss 203.909\n",
      "step 47300, train accuracy 0.872, train loss 197.847\n",
      "step 47400, train accuracy 0.88, train loss 189.642\n",
      "step 47500, train accuracy 0.842, train loss 207.621\n",
      "step 47600, train accuracy 0.884, train loss 167.746\n",
      "step 47700, train accuracy 0.862, train loss 207.725\n",
      "step 47800, train accuracy 0.858, train loss 201.764\n",
      "step 47900, train accuracy 0.83, train loss 233.779\n",
      "step 48000, train accuracy 0.868, train loss 203.542\n",
      "step 48100, train accuracy 0.862, train loss 212.257\n",
      "step 48200, train accuracy 0.876, train loss 192.861\n",
      "step 48300, train accuracy 0.88, train loss 187.957\n",
      "step 48400, train accuracy 0.838, train loss 235.021\n",
      "step 48500, train accuracy 0.86, train loss 197.786\n",
      "step 48600, train accuracy 0.878, train loss 200.138\n",
      "step 48700, train accuracy 0.856, train loss 202.736\n",
      "step 48800, train accuracy 0.876, train loss 193.285\n",
      "step 48900, train accuracy 0.856, train loss 203.525\n",
      "step 49000, train accuracy 0.86, train loss 200.618\n",
      "step 49100, train accuracy 0.868, train loss 191.742\n",
      "step 49200, train accuracy 0.902, train loss 170.51\n",
      "step 49300, train accuracy 0.866, train loss 191.758\n",
      "step 49400, train accuracy 0.858, train loss 211.804\n",
      "step 49500, train accuracy 0.862, train loss 186.952\n",
      "step 49600, train accuracy 0.882, train loss 176.521\n",
      "step 49700, train accuracy 0.876, train loss 195.388\n",
      "step 49800, train accuracy 0.85, train loss 213.353\n",
      "step 49900, train accuracy 0.886, train loss 181.087\n",
      "step 50000, train accuracy 0.858, train loss 193.374\n",
      "step 50100, train accuracy 0.846, train loss 232.043\n",
      "step 50200, train accuracy 0.882, train loss 174.412\n",
      "step 50300, train accuracy 0.86, train loss 199.98\n",
      "step 50400, train accuracy 0.866, train loss 190.701\n",
      "step 50500, train accuracy 0.882, train loss 184.157\n",
      "step 50600, train accuracy 0.866, train loss 206.876\n",
      "step 50700, train accuracy 0.878, train loss 173.938\n",
      "step 50800, train accuracy 0.878, train loss 209.032\n",
      "step 50900, train accuracy 0.862, train loss 188.63\n",
      "step 51000, train accuracy 0.87, train loss 211.162\n",
      "step 51100, train accuracy 0.85, train loss 198.371\n",
      "step 51200, train accuracy 0.842, train loss 215.129\n",
      "step 51300, train accuracy 0.89, train loss 172.324\n",
      "step 51400, train accuracy 0.872, train loss 184.86\n",
      "step 51500, train accuracy 0.89, train loss 166.634\n",
      "step 51600, train accuracy 0.86, train loss 189.489\n",
      "step 51700, train accuracy 0.91, train loss 160.718\n",
      "step 51800, train accuracy 0.902, train loss 154.762\n",
      "step 51900, train accuracy 0.852, train loss 203.855\n",
      "step 52000, train accuracy 0.85, train loss 204.098\n",
      "step 52100, train accuracy 0.88, train loss 175.885\n",
      "step 52200, train accuracy 0.868, train loss 191.17\n",
      "step 52300, train accuracy 0.86, train loss 200.169\n",
      "step 52400, train accuracy 0.882, train loss 208.009\n",
      "step 52500, train accuracy 0.904, train loss 161.12\n",
      "step 52600, train accuracy 0.898, train loss 178.427\n",
      "step 52700, train accuracy 0.886, train loss 172.479\n",
      "step 52800, train accuracy 0.876, train loss 168.713\n",
      "step 52900, train accuracy 0.914, train loss 155.889\n",
      "step 53000, train accuracy 0.882, train loss 168.87\n",
      "step 53100, train accuracy 0.892, train loss 176.168\n",
      "step 53200, train accuracy 0.89, train loss 162.189\n",
      "step 53300, train accuracy 0.86, train loss 194.733\n",
      "step 53400, train accuracy 0.902, train loss 166.896\n",
      "step 53500, train accuracy 0.888, train loss 179.219\n",
      "step 53600, train accuracy 0.876, train loss 186.306\n",
      "step 53700, train accuracy 0.866, train loss 190.98\n",
      "step 53800, train accuracy 0.882, train loss 182.287\n",
      "step 53900, train accuracy 0.888, train loss 178.592\n",
      "step 54000, train accuracy 0.884, train loss 177.378\n",
      "step 54100, train accuracy 0.892, train loss 168.459\n",
      "step 54200, train accuracy 0.876, train loss 204.208\n",
      "step 54300, train accuracy 0.868, train loss 187.346\n",
      "step 54400, train accuracy 0.9, train loss 146.136\n",
      "step 54500, train accuracy 0.886, train loss 179.774\n",
      "step 54600, train accuracy 0.862, train loss 171.477\n",
      "step 54700, train accuracy 0.858, train loss 186.229\n",
      "step 54800, train accuracy 0.876, train loss 179.034\n",
      "step 54900, train accuracy 0.888, train loss 179.93\n",
      "step 55000, train accuracy 0.894, train loss 159.068\n",
      "step 55100, train accuracy 0.876, train loss 170.41\n",
      "step 55200, train accuracy 0.896, train loss 168.766\n",
      "step 55300, train accuracy 0.898, train loss 165.428\n",
      "step 55400, train accuracy 0.89, train loss 161.053\n",
      "step 55500, train accuracy 0.884, train loss 160.709\n",
      "step 55600, train accuracy 0.878, train loss 171.891\n",
      "step 55700, train accuracy 0.89, train loss 170.267\n",
      "step 55800, train accuracy 0.848, train loss 187.725\n",
      "step 55900, train accuracy 0.898, train loss 160.062\n",
      "step 56000, train accuracy 0.89, train loss 170.867\n",
      "step 56100, train accuracy 0.884, train loss 180.642\n",
      "step 56200, train accuracy 0.898, train loss 158.613\n",
      "step 56300, train accuracy 0.888, train loss 177.542\n",
      "step 56400, train accuracy 0.89, train loss 161.317\n",
      "step 56500, train accuracy 0.88, train loss 194.285\n",
      "step 56600, train accuracy 0.882, train loss 177.176\n",
      "step 56700, train accuracy 0.88, train loss 174.353\n",
      "step 56800, train accuracy 0.904, train loss 152.176\n",
      "step 56900, train accuracy 0.92, train loss 139.789\n",
      "step 57000, train accuracy 0.892, train loss 164.301\n",
      "step 57100, train accuracy 0.894, train loss 163.765\n",
      "step 57200, train accuracy 0.898, train loss 148.93\n",
      "step 57300, train accuracy 0.884, train loss 178.144\n",
      "step 57400, train accuracy 0.908, train loss 164.334\n",
      "step 57500, train accuracy 0.87, train loss 184.186\n",
      "step 57600, train accuracy 0.898, train loss 154.71\n",
      "step 57700, train accuracy 0.88, train loss 162.408\n",
      "step 57800, train accuracy 0.894, train loss 158.39\n",
      "step 57900, train accuracy 0.898, train loss 168.739\n",
      "step 58000, train accuracy 0.912, train loss 144.911\n",
      "step 58100, train accuracy 0.912, train loss 159.185\n",
      "step 58200, train accuracy 0.884, train loss 151.178\n",
      "step 58300, train accuracy 0.856, train loss 190.48\n",
      "step 58400, train accuracy 0.882, train loss 156.617\n",
      "step 58500, train accuracy 0.88, train loss 164.54\n",
      "step 58600, train accuracy 0.874, train loss 180.733\n",
      "step 58700, train accuracy 0.89, train loss 159.359\n",
      "step 58800, train accuracy 0.894, train loss 155.523\n",
      "step 58900, train accuracy 0.906, train loss 144.232\n",
      "step 59000, train accuracy 0.874, train loss 180.934\n",
      "step 59100, train accuracy 0.912, train loss 145.995\n",
      "step 59200, train accuracy 0.924, train loss 136.149\n",
      "step 59300, train accuracy 0.922, train loss 139.294\n",
      "step 59400, train accuracy 0.916, train loss 133.705\n",
      "step 59500, train accuracy 0.908, train loss 163.461\n",
      "step 59600, train accuracy 0.908, train loss 161.794\n",
      "step 59700, train accuracy 0.902, train loss 158.942\n",
      "step 59800, train accuracy 0.906, train loss 153.194\n",
      "step 59900, train accuracy 0.896, train loss 165.454\n",
      "step 60000, train accuracy 0.884, train loss 160.09\n",
      "step 60100, train accuracy 0.898, train loss 141.147\n",
      "step 60200, train accuracy 0.91, train loss 158.712\n",
      "step 60300, train accuracy 0.908, train loss 137.741\n",
      "step 60400, train accuracy 0.92, train loss 144.387\n",
      "step 60500, train accuracy 0.892, train loss 162.63\n",
      "step 60600, train accuracy 0.906, train loss 141.491\n",
      "step 60700, train accuracy 0.894, train loss 175.067\n",
      "step 60800, train accuracy 0.892, train loss 167.283\n",
      "step 60900, train accuracy 0.902, train loss 165.061\n",
      "step 61000, train accuracy 0.906, train loss 165.146\n",
      "step 61100, train accuracy 0.894, train loss 161.441\n",
      "step 61200, train accuracy 0.896, train loss 156.792\n",
      "step 61300, train accuracy 0.9, train loss 148.69\n",
      "step 61400, train accuracy 0.904, train loss 149.969\n",
      "step 61500, train accuracy 0.896, train loss 164.248\n",
      "step 61600, train accuracy 0.862, train loss 193.235\n",
      "step 61700, train accuracy 0.918, train loss 134.872\n",
      "step 61800, train accuracy 0.9, train loss 149.11\n",
      "step 61900, train accuracy 0.894, train loss 155.172\n",
      "step 62000, train accuracy 0.894, train loss 142.001\n",
      "step 62100, train accuracy 0.918, train loss 143.476\n",
      "step 62200, train accuracy 0.89, train loss 173.164\n",
      "step 62300, train accuracy 0.9, train loss 153.666\n",
      "step 62400, train accuracy 0.898, train loss 165.344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62500, train accuracy 0.896, train loss 133.829\n",
      "step 62600, train accuracy 0.928, train loss 131.318\n",
      "step 62700, train accuracy 0.904, train loss 140.176\n",
      "step 62800, train accuracy 0.87, train loss 183.127\n",
      "step 62900, train accuracy 0.886, train loss 151.287\n",
      "step 63000, train accuracy 0.892, train loss 181.156\n",
      "step 63100, train accuracy 0.9, train loss 151.837\n",
      "step 63200, train accuracy 0.922, train loss 130.446\n",
      "step 63300, train accuracy 0.898, train loss 150.232\n",
      "step 63400, train accuracy 0.928, train loss 118.445\n",
      "step 63500, train accuracy 0.908, train loss 174.713\n",
      "step 63600, train accuracy 0.89, train loss 164.174\n",
      "step 63700, train accuracy 0.904, train loss 159.902\n",
      "step 63800, train accuracy 0.91, train loss 173.268\n",
      "step 63900, train accuracy 0.872, train loss 181.654\n",
      "step 64000, train accuracy 0.92, train loss 128.28\n",
      "step 64100, train accuracy 0.918, train loss 136.29\n",
      "step 64200, train accuracy 0.908, train loss 138.953\n",
      "step 64300, train accuracy 0.912, train loss 133.628\n",
      "step 64400, train accuracy 0.898, train loss 173.382\n",
      "step 64500, train accuracy 0.924, train loss 124.801\n",
      "step 64600, train accuracy 0.914, train loss 145.314\n",
      "step 64700, train accuracy 0.886, train loss 161.361\n",
      "step 64800, train accuracy 0.912, train loss 134.801\n",
      "step 64900, train accuracy 0.92, train loss 136.263\n",
      "step 65000, train accuracy 0.91, train loss 125.105\n",
      "step 65100, train accuracy 0.878, train loss 176.121\n",
      "step 65200, train accuracy 0.932, train loss 112.358\n",
      "step 65300, train accuracy 0.896, train loss 148.183\n",
      "step 65400, train accuracy 0.916, train loss 141.107\n",
      "step 65500, train accuracy 0.916, train loss 130.438\n",
      "step 65600, train accuracy 0.91, train loss 151.033\n",
      "step 65700, train accuracy 0.922, train loss 118.942\n",
      "step 65800, train accuracy 0.934, train loss 112.266\n",
      "step 65900, train accuracy 0.898, train loss 153.747\n",
      "step 66000, train accuracy 0.898, train loss 160.347\n",
      "step 66100, train accuracy 0.906, train loss 135.717\n",
      "step 66200, train accuracy 0.91, train loss 133.009\n",
      "step 66300, train accuracy 0.892, train loss 155.347\n",
      "step 66400, train accuracy 0.916, train loss 139.037\n",
      "step 66500, train accuracy 0.92, train loss 130.973\n",
      "step 66600, train accuracy 0.934, train loss 117.439\n",
      "step 66700, train accuracy 0.894, train loss 137.008\n",
      "step 66800, train accuracy 0.906, train loss 136.747\n",
      "step 66900, train accuracy 0.932, train loss 123.192\n",
      "step 67000, train accuracy 0.902, train loss 152.935\n",
      "step 67100, train accuracy 0.914, train loss 152.963\n",
      "step 67200, train accuracy 0.91, train loss 129.991\n",
      "step 67300, train accuracy 0.932, train loss 118.184\n",
      "step 67400, train accuracy 0.9, train loss 140.583\n",
      "step 67500, train accuracy 0.908, train loss 142.366\n",
      "step 67600, train accuracy 0.932, train loss 130.313\n",
      "step 67700, train accuracy 0.934, train loss 128.869\n",
      "step 67800, train accuracy 0.922, train loss 123.491\n",
      "step 67900, train accuracy 0.922, train loss 129.598\n",
      "step 68000, train accuracy 0.922, train loss 121.751\n",
      "step 68100, train accuracy 0.914, train loss 128.093\n",
      "step 68200, train accuracy 0.92, train loss 122.369\n",
      "step 68300, train accuracy 0.94, train loss 113.739\n",
      "step 68400, train accuracy 0.926, train loss 124.044\n",
      "step 68500, train accuracy 0.924, train loss 130.569\n",
      "step 68600, train accuracy 0.948, train loss 104.766\n",
      "step 68700, train accuracy 0.914, train loss 134.901\n",
      "step 68800, train accuracy 0.922, train loss 121.066\n",
      "step 68900, train accuracy 0.928, train loss 122.489\n",
      "step 69000, train accuracy 0.914, train loss 136.279\n",
      "step 69100, train accuracy 0.926, train loss 108.731\n",
      "step 69200, train accuracy 0.91, train loss 125.419\n",
      "step 69300, train accuracy 0.95, train loss 104.6\n",
      "step 69400, train accuracy 0.9, train loss 140.307\n",
      "step 69500, train accuracy 0.922, train loss 124.053\n",
      "step 69600, train accuracy 0.894, train loss 162.863\n",
      "step 69700, train accuracy 0.926, train loss 110.6\n",
      "step 69800, train accuracy 0.866, train loss 188.176\n",
      "step 69900, train accuracy 0.928, train loss 111.117\n",
      "step 70000, train accuracy 0.9, train loss 142.494\n",
      "step 70100, train accuracy 0.908, train loss 136.875\n",
      "step 70200, train accuracy 0.928, train loss 123.269\n",
      "step 70300, train accuracy 0.944, train loss 110.143\n",
      "step 70400, train accuracy 0.914, train loss 129.281\n",
      "step 70500, train accuracy 0.952, train loss 94.027\n",
      "step 70600, train accuracy 0.916, train loss 129.167\n",
      "step 70700, train accuracy 0.936, train loss 118.193\n",
      "step 70800, train accuracy 0.936, train loss 98.439\n",
      "step 70900, train accuracy 0.916, train loss 122.287\n",
      "step 71000, train accuracy 0.934, train loss 123.084\n",
      "step 71100, train accuracy 0.922, train loss 122.259\n",
      "step 71200, train accuracy 0.926, train loss 130.35\n",
      "step 71300, train accuracy 0.916, train loss 126.069\n",
      "step 71400, train accuracy 0.896, train loss 125.27\n",
      "step 71500, train accuracy 0.932, train loss 115.294\n",
      "step 71600, train accuracy 0.918, train loss 136.861\n",
      "step 71700, train accuracy 0.936, train loss 119.841\n",
      "step 71800, train accuracy 0.922, train loss 122.185\n",
      "step 71900, train accuracy 0.86, train loss 180.972\n",
      "step 72000, train accuracy 0.892, train loss 156.08\n",
      "step 72100, train accuracy 0.914, train loss 125.835\n",
      "step 72200, train accuracy 0.938, train loss 113.27\n",
      "step 72300, train accuracy 0.914, train loss 122.855\n",
      "step 72400, train accuracy 0.912, train loss 119.529\n",
      "step 72500, train accuracy 0.932, train loss 118.464\n",
      "step 72600, train accuracy 0.912, train loss 143.449\n",
      "step 72700, train accuracy 0.922, train loss 120.865\n",
      "step 72800, train accuracy 0.862, train loss 182.478\n",
      "step 72900, train accuracy 0.91, train loss 137.247\n",
      "step 73000, train accuracy 0.948, train loss 102.645\n",
      "step 73100, train accuracy 0.946, train loss 103.884\n",
      "step 73200, train accuracy 0.912, train loss 121.591\n",
      "step 73300, train accuracy 0.93, train loss 110.063\n",
      "step 73400, train accuracy 0.892, train loss 153.442\n",
      "step 73500, train accuracy 0.93, train loss 123.439\n",
      "step 73600, train accuracy 0.854, train loss 210.535\n",
      "step 73700, train accuracy 0.91, train loss 122.577\n",
      "step 73800, train accuracy 0.922, train loss 112.074\n",
      "step 73900, train accuracy 0.934, train loss 112.201\n",
      "step 74000, train accuracy 0.95, train loss 101.324\n",
      "step 74100, train accuracy 0.928, train loss 114.854\n",
      "step 74200, train accuracy 0.946, train loss 104.126\n",
      "step 74300, train accuracy 0.932, train loss 109.037\n",
      "step 74400, train accuracy 0.932, train loss 113.032\n",
      "step 74500, train accuracy 0.934, train loss 98.2428\n",
      "step 74600, train accuracy 0.942, train loss 101.269\n",
      "step 74700, train accuracy 0.93, train loss 102.651\n",
      "step 74800, train accuracy 0.914, train loss 121.556\n",
      "step 74900, train accuracy 0.936, train loss 118.404\n",
      "step 75000, train accuracy 0.92, train loss 121.254\n",
      "step 75100, train accuracy 0.926, train loss 113.125\n",
      "step 75200, train accuracy 0.906, train loss 134.272\n",
      "step 75300, train accuracy 0.916, train loss 126.351\n",
      "step 75400, train accuracy 0.95, train loss 95.457\n",
      "step 75500, train accuracy 0.924, train loss 119.297\n",
      "step 75600, train accuracy 0.918, train loss 132.29\n",
      "step 75700, train accuracy 0.902, train loss 137.317\n",
      "step 75800, train accuracy 0.94, train loss 108.865\n",
      "step 75900, train accuracy 0.93, train loss 105.945\n",
      "step 76000, train accuracy 0.946, train loss 117.018\n",
      "step 76100, train accuracy 0.916, train loss 120.253\n",
      "step 76200, train accuracy 0.934, train loss 118.883\n",
      "step 76300, train accuracy 0.932, train loss 116.408\n",
      "step 76400, train accuracy 0.93, train loss 108.125\n",
      "step 76500, train accuracy 0.924, train loss 120.561\n",
      "step 76600, train accuracy 0.92, train loss 118.588\n",
      "step 76700, train accuracy 0.936, train loss 111.735\n",
      "step 76800, train accuracy 0.904, train loss 133.884\n",
      "step 76900, train accuracy 0.922, train loss 116.936\n",
      "step 77000, train accuracy 0.922, train loss 115.15\n",
      "step 77100, train accuracy 0.938, train loss 95.0335\n",
      "step 77200, train accuracy 0.93, train loss 117.421\n",
      "step 77300, train accuracy 0.924, train loss 122.048\n",
      "step 77400, train accuracy 0.926, train loss 116.575\n",
      "step 77500, train accuracy 0.928, train loss 111.942\n",
      "step 77600, train accuracy 0.948, train loss 123.492\n",
      "step 77700, train accuracy 0.936, train loss 101.515\n",
      "step 77800, train accuracy 0.942, train loss 99.5298\n",
      "step 77900, train accuracy 0.95, train loss 89.9688\n",
      "step 78000, train accuracy 0.948, train loss 94.1216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 78100, train accuracy 0.932, train loss 103.511\n",
      "step 78200, train accuracy 0.918, train loss 123.106\n",
      "step 78300, train accuracy 0.94, train loss 111.036\n",
      "step 78400, train accuracy 0.926, train loss 123.112\n",
      "step 78500, train accuracy 0.946, train loss 92.3723\n",
      "step 78600, train accuracy 0.942, train loss 104.194\n",
      "step 78700, train accuracy 0.944, train loss 99.5073\n",
      "step 78800, train accuracy 0.934, train loss 115.593\n",
      "step 78900, train accuracy 0.952, train loss 107.338\n",
      "step 79000, train accuracy 0.948, train loss 91.2885\n",
      "step 79100, train accuracy 0.924, train loss 130.424\n",
      "step 79200, train accuracy 0.932, train loss 111.816\n",
      "step 79300, train accuracy 0.934, train loss 115.441\n",
      "step 79400, train accuracy 0.932, train loss 98.9997\n",
      "step 79500, train accuracy 0.952, train loss 87.3668\n",
      "step 79600, train accuracy 0.916, train loss 130.997\n",
      "step 79700, train accuracy 0.948, train loss 96.1721\n",
      "step 79800, train accuracy 0.938, train loss 115.567\n",
      "step 79900, train accuracy 0.942, train loss 106.36\n",
      "step 80000, train accuracy 0.92, train loss 123.165\n",
      "step 80100, train accuracy 0.908, train loss 129.189\n",
      "step 80200, train accuracy 0.942, train loss 99.6789\n",
      "step 80300, train accuracy 0.926, train loss 109.379\n",
      "step 80400, train accuracy 0.906, train loss 131.906\n",
      "step 80500, train accuracy 0.952, train loss 97.3899\n",
      "step 80600, train accuracy 0.918, train loss 134.825\n",
      "step 80700, train accuracy 0.938, train loss 100.178\n",
      "step 80800, train accuracy 0.924, train loss 114.807\n",
      "step 80900, train accuracy 0.938, train loss 104.468\n",
      "step 81000, train accuracy 0.916, train loss 128.335\n",
      "step 81100, train accuracy 0.958, train loss 89.9519\n",
      "step 81200, train accuracy 0.948, train loss 104.06\n",
      "step 81300, train accuracy 0.906, train loss 125.032\n",
      "step 81400, train accuracy 0.952, train loss 93.2683\n",
      "step 81500, train accuracy 0.938, train loss 100.81\n",
      "step 81600, train accuracy 0.958, train loss 82.3237\n",
      "step 81700, train accuracy 0.956, train loss 84.8443\n",
      "step 81800, train accuracy 0.958, train loss 77.7713\n",
      "step 81900, train accuracy 0.96, train loss 75.6581\n",
      "step 82000, train accuracy 0.962, train loss 78.2308\n",
      "step 82100, train accuracy 0.95, train loss 95.2352\n",
      "step 82200, train accuracy 0.942, train loss 97.3014\n",
      "step 82300, train accuracy 0.956, train loss 93.1353\n",
      "step 82400, train accuracy 0.96, train loss 85.2675\n",
      "step 82500, train accuracy 0.946, train loss 93.6207\n",
      "step 82600, train accuracy 0.938, train loss 105.525\n",
      "step 82700, train accuracy 0.958, train loss 90.2072\n",
      "step 82800, train accuracy 0.944, train loss 87.5671\n",
      "step 82900, train accuracy 0.94, train loss 121.055\n",
      "step 83000, train accuracy 0.938, train loss 99.4803\n",
      "step 83100, train accuracy 0.958, train loss 83.9108\n",
      "step 83200, train accuracy 0.938, train loss 90.8805\n",
      "step 83300, train accuracy 0.942, train loss 97.1653\n",
      "step 83400, train accuracy 0.942, train loss 95.982\n",
      "step 83500, train accuracy 0.95, train loss 84.3128\n",
      "step 83600, train accuracy 0.95, train loss 91.9818\n",
      "step 83700, train accuracy 0.942, train loss 100.616\n",
      "step 83800, train accuracy 0.944, train loss 99.0358\n",
      "step 83900, train accuracy 0.942, train loss 93.1862\n",
      "step 84000, train accuracy 0.95, train loss 90.0174\n",
      "step 84100, train accuracy 0.956, train loss 84.2011\n",
      "step 84200, train accuracy 0.92, train loss 125.9\n",
      "step 84300, train accuracy 0.962, train loss 76.4425\n",
      "step 84400, train accuracy 0.936, train loss 103.564\n",
      "step 84500, train accuracy 0.944, train loss 90.1355\n",
      "step 84600, train accuracy 0.938, train loss 102.599\n",
      "step 84700, train accuracy 0.938, train loss 99.332\n",
      "step 84800, train accuracy 0.948, train loss 105.149\n",
      "step 84900, train accuracy 0.962, train loss 70.6652\n",
      "step 85000, train accuracy 0.95, train loss 83.4495\n",
      "step 85100, train accuracy 0.926, train loss 106.424\n",
      "step 85200, train accuracy 0.944, train loss 93.2398\n",
      "step 85300, train accuracy 0.944, train loss 111.247\n",
      "step 85400, train accuracy 0.94, train loss 119.299\n",
      "step 85500, train accuracy 0.938, train loss 113.618\n",
      "step 85600, train accuracy 0.968, train loss 78.4224\n",
      "step 85700, train accuracy 0.946, train loss 92.7574\n",
      "step 85800, train accuracy 0.944, train loss 89.2224\n",
      "step 85900, train accuracy 0.948, train loss 91.9873\n",
      "step 86000, train accuracy 0.952, train loss 91.2138\n",
      "step 86100, train accuracy 0.944, train loss 101.873\n",
      "step 86200, train accuracy 0.942, train loss 88.1718\n",
      "step 86300, train accuracy 0.942, train loss 96.8184\n",
      "step 86400, train accuracy 0.952, train loss 84.9783\n",
      "step 86500, train accuracy 0.968, train loss 65.6821\n",
      "step 86600, train accuracy 0.942, train loss 88.826\n",
      "step 86700, train accuracy 0.942, train loss 93.5796\n",
      "step 86800, train accuracy 0.938, train loss 95.5349\n",
      "step 86900, train accuracy 0.962, train loss 73.8611\n",
      "step 87000, train accuracy 0.958, train loss 81.2465\n",
      "step 87100, train accuracy 0.954, train loss 85.9462\n",
      "step 87200, train accuracy 0.938, train loss 92.6173\n",
      "step 87300, train accuracy 0.95, train loss 86.4992\n",
      "step 87400, train accuracy 0.942, train loss 90.4326\n",
      "step 87500, train accuracy 0.938, train loss 96.0133\n",
      "step 87600, train accuracy 0.958, train loss 85.655\n",
      "step 87700, train accuracy 0.926, train loss 100.803\n",
      "step 87800, train accuracy 0.956, train loss 69.8126\n",
      "step 87900, train accuracy 0.956, train loss 86.0424\n",
      "step 88000, train accuracy 0.948, train loss 89.0337\n",
      "step 88100, train accuracy 0.952, train loss 84.274\n",
      "step 88200, train accuracy 0.966, train loss 82.0913\n",
      "step 88300, train accuracy 0.958, train loss 93.1555\n",
      "step 88400, train accuracy 0.962, train loss 75.5053\n",
      "step 88500, train accuracy 0.95, train loss 94.4569\n",
      "step 88600, train accuracy 0.94, train loss 87.6919\n",
      "step 88700, train accuracy 0.942, train loss 93.1144\n",
      "step 88800, train accuracy 0.956, train loss 75.5032\n",
      "step 88900, train accuracy 0.964, train loss 77.6279\n",
      "step 89000, train accuracy 0.958, train loss 82.5141\n",
      "step 89100, train accuracy 0.944, train loss 80.2948\n",
      "step 89200, train accuracy 0.958, train loss 79.9503\n",
      "step 89300, train accuracy 0.95, train loss 89.8446\n",
      "step 89400, train accuracy 0.968, train loss 65.8684\n",
      "step 89500, train accuracy 0.952, train loss 92.3025\n",
      "step 89600, train accuracy 0.936, train loss 104.418\n",
      "step 89700, train accuracy 0.952, train loss 86.3789\n",
      "step 89800, train accuracy 0.95, train loss 92.1361\n",
      "step 89900, train accuracy 0.956, train loss 77.9674\n",
      "step 90000, train accuracy 0.97, train loss 66.1353\n",
      "step 90100, train accuracy 0.972, train loss 67.7316\n",
      "step 90200, train accuracy 0.956, train loss 77.5907\n",
      "step 90300, train accuracy 0.948, train loss 79.9005\n",
      "step 90400, train accuracy 0.954, train loss 80.0845\n",
      "step 90500, train accuracy 0.96, train loss 74.8082\n",
      "step 90600, train accuracy 0.946, train loss 84.3402\n",
      "step 90700, train accuracy 0.946, train loss 86.1818\n",
      "step 90800, train accuracy 0.964, train loss 76.9535\n",
      "step 90900, train accuracy 0.966, train loss 66.7305\n",
      "step 91000, train accuracy 0.966, train loss 61.512\n",
      "step 91100, train accuracy 0.912, train loss 145.012\n",
      "step 91200, train accuracy 0.964, train loss 67.8763\n",
      "step 91300, train accuracy 0.93, train loss 111.704\n",
      "step 91400, train accuracy 0.944, train loss 85.0814\n",
      "step 91500, train accuracy 0.964, train loss 70.0908\n",
      "step 91600, train accuracy 0.956, train loss 73.1822\n",
      "step 91700, train accuracy 0.952, train loss 89.3883\n",
      "step 91800, train accuracy 0.964, train loss 80.0367\n",
      "step 91900, train accuracy 0.966, train loss 82.8903\n",
      "step 92000, train accuracy 0.958, train loss 82.6165\n",
      "step 92100, train accuracy 0.938, train loss 97.9164\n",
      "step 92200, train accuracy 0.952, train loss 75.0887\n",
      "step 92300, train accuracy 0.96, train loss 76.5213\n",
      "step 92400, train accuracy 0.952, train loss 89.9464\n",
      "step 92500, train accuracy 0.95, train loss 90.1642\n",
      "step 92600, train accuracy 0.956, train loss 78.2615\n",
      "step 92700, train accuracy 0.972, train loss 66.7413\n",
      "step 92800, train accuracy 0.966, train loss 70.109\n",
      "step 92900, train accuracy 0.958, train loss 77.669\n",
      "step 93000, train accuracy 0.96, train loss 72.0681\n",
      "step 93100, train accuracy 0.962, train loss 63.4467\n",
      "step 93200, train accuracy 0.942, train loss 88.3861\n",
      "step 93300, train accuracy 0.948, train loss 84.2605\n",
      "step 93400, train accuracy 0.956, train loss 86.2388\n",
      "step 93500, train accuracy 0.96, train loss 72.2227\n",
      "step 93600, train accuracy 0.96, train loss 69.9639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 93700, train accuracy 0.978, train loss 50.352\n",
      "step 93800, train accuracy 0.958, train loss 72.8953\n",
      "step 93900, train accuracy 0.952, train loss 70.6434\n",
      "step 94000, train accuracy 0.914, train loss 112.749\n",
      "step 94100, train accuracy 0.966, train loss 69.451\n",
      "step 94200, train accuracy 0.96, train loss 78.8697\n",
      "step 94300, train accuracy 0.972, train loss 66.3173\n",
      "step 94400, train accuracy 0.952, train loss 73.374\n",
      "step 94500, train accuracy 0.96, train loss 73.1296\n",
      "step 94600, train accuracy 0.95, train loss 89.1288\n",
      "step 94700, train accuracy 0.956, train loss 73.5389\n",
      "step 94800, train accuracy 0.942, train loss 77.2799\n",
      "step 94900, train accuracy 0.956, train loss 90.9083\n",
      "step 95000, train accuracy 0.96, train loss 78.7145\n",
      "step 95100, train accuracy 0.952, train loss 82.0504\n",
      "step 95200, train accuracy 0.938, train loss 91.4984\n",
      "step 95300, train accuracy 0.95, train loss 75.2883\n",
      "step 95400, train accuracy 0.916, train loss 132.086\n",
      "step 95500, train accuracy 0.974, train loss 69.14\n",
      "step 95600, train accuracy 0.962, train loss 72.3064\n",
      "step 95700, train accuracy 0.968, train loss 66.2492\n",
      "step 95800, train accuracy 0.964, train loss 68.5197\n",
      "step 95900, train accuracy 0.98, train loss 63.4007\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data_shuffle.npy'\n",
    "vali_filename = 'E:/Alibaba German AI Challenge/data_process/sample_of_training.npy'\n",
    "data = np.load(filename)\n",
    "vali = np.load(vali_filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "print('The shape of vali is ',vali.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# flatten\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "#Eval = []\n",
    "\n",
    "batch_size = 500\n",
    "for i in range(300000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    temp_loss = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "#    vali_accuracy = accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "    Loss.append(temp_loss)\n",
    "#    Eval.append(vali_accuracy)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %g\" %(i, train_accuracy, temp_loss))\n",
    "        if train_accuracy > 0.98:\n",
    "            break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.85      0.51        71\n",
      "          1       0.65      0.58      0.61       342\n",
      "          2       0.81      0.56      0.66       474\n",
      "          3       0.42      0.72      0.53       119\n",
      "          4       0.41      0.63      0.50       217\n",
      "          5       0.70      0.60      0.65       487\n",
      "          6       0.50      0.93      0.65        40\n",
      "          7       0.85      0.46      0.60       586\n",
      "          8       0.55      0.65      0.60       188\n",
      "          9       0.33      0.59      0.42       158\n",
      "         10       0.96      0.79      0.86       622\n",
      "         11       0.33      0.86      0.48       125\n",
      "         12       0.58      0.83      0.69       134\n",
      "         13       0.87      0.55      0.67       585\n",
      "         14       0.41      1.00      0.58        26\n",
      "         15       0.51      0.96      0.67        91\n",
      "         16       0.98      0.97      0.98       735\n",
      "\n",
      "avg / total       0.76      0.68      0.70      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = y_conv.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "train_val_y = np.argmax(vali[:,-17:],axis = 1)\n",
    "pred_y = np.argmax(pred, axis = 1)\n",
    "print (classification_report(train_val_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('seventh_batch_shuffle_balance_50k_train_SGD.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAE/CAYAAABM7Bo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+iIiIhGh0BcREYkIhb6IiEhE/H8CDBd+dmR+kgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x144ab305b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_data : shuffle 50k\n",
    "* vali_data : 5k\n",
    "* batch_size : 500\n",
    "* Optimizer : Adam\n",
    "* learning_rate : 1e-4\n",
    "* accuracy_threshold : 0.98\n",
    "* step_limit : 300K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "The shape of vali is  (5000, 18449)\n",
      "step 0, train accuracy 0.092, train loss 5567.23\n",
      "step 100, train accuracy 0.276, train loss 1896.27\n",
      "step 200, train accuracy 0.366, train loss 1441.8\n",
      "step 300, train accuracy 0.384, train loss 1312.48\n",
      "step 400, train accuracy 0.372, train loss 1222.29\n",
      "step 500, train accuracy 0.412, train loss 1060.14\n",
      "step 600, train accuracy 0.43, train loss 1057.79\n",
      "step 700, train accuracy 0.43, train loss 993.534\n",
      "step 800, train accuracy 0.424, train loss 1009.01\n",
      "step 900, train accuracy 0.466, train loss 944.316\n",
      "step 1000, train accuracy 0.446, train loss 843.791\n",
      "step 1100, train accuracy 0.482, train loss 813.107\n",
      "step 1200, train accuracy 0.494, train loss 854.486\n",
      "step 1300, train accuracy 0.44, train loss 919.483\n",
      "step 1400, train accuracy 0.456, train loss 861.855\n",
      "step 1500, train accuracy 0.484, train loss 795.66\n",
      "step 1600, train accuracy 0.502, train loss 789.829\n",
      "step 1700, train accuracy 0.522, train loss 762.119\n",
      "step 1800, train accuracy 0.482, train loss 831.998\n",
      "step 1900, train accuracy 0.524, train loss 783.912\n",
      "step 2000, train accuracy 0.492, train loss 783.383\n",
      "step 2100, train accuracy 0.506, train loss 722.527\n",
      "step 2200, train accuracy 0.568, train loss 713.771\n",
      "step 2300, train accuracy 0.518, train loss 708.32\n",
      "step 2400, train accuracy 0.508, train loss 686.244\n",
      "step 2500, train accuracy 0.556, train loss 671.072\n",
      "step 2600, train accuracy 0.544, train loss 651.67\n",
      "step 2700, train accuracy 0.546, train loss 679.998\n",
      "step 2800, train accuracy 0.544, train loss 684.904\n",
      "step 2900, train accuracy 0.592, train loss 663.137\n",
      "step 3000, train accuracy 0.53, train loss 713.233\n",
      "step 3100, train accuracy 0.61, train loss 671.416\n",
      "step 3200, train accuracy 0.562, train loss 618.156\n",
      "step 3300, train accuracy 0.582, train loss 610.97\n",
      "step 3400, train accuracy 0.592, train loss 606.041\n",
      "step 3500, train accuracy 0.612, train loss 664.372\n",
      "step 3600, train accuracy 0.548, train loss 624.778\n",
      "step 3700, train accuracy 0.532, train loss 679.926\n",
      "step 3800, train accuracy 0.568, train loss 680.844\n",
      "step 3900, train accuracy 0.574, train loss 634.606\n",
      "step 4000, train accuracy 0.606, train loss 605.828\n",
      "step 4100, train accuracy 0.55, train loss 644.618\n",
      "step 4200, train accuracy 0.628, train loss 577.102\n",
      "step 4300, train accuracy 0.596, train loss 630.79\n",
      "step 4400, train accuracy 0.622, train loss 562.728\n",
      "step 4500, train accuracy 0.6, train loss 608.231\n",
      "step 4600, train accuracy 0.6, train loss 611.213\n",
      "step 4700, train accuracy 0.614, train loss 551.197\n",
      "step 4800, train accuracy 0.636, train loss 551.68\n",
      "step 4900, train accuracy 0.604, train loss 599.455\n",
      "step 5000, train accuracy 0.64, train loss 570.07\n",
      "step 5100, train accuracy 0.59, train loss 590.51\n",
      "step 5200, train accuracy 0.654, train loss 534.625\n",
      "step 5300, train accuracy 0.61, train loss 558.6\n",
      "step 5400, train accuracy 0.646, train loss 584.18\n",
      "step 5500, train accuracy 0.66, train loss 502.172\n",
      "step 5600, train accuracy 0.652, train loss 564.993\n",
      "step 5700, train accuracy 0.622, train loss 571.112\n",
      "step 5800, train accuracy 0.612, train loss 603.001\n",
      "step 5900, train accuracy 0.638, train loss 500.256\n",
      "step 6000, train accuracy 0.668, train loss 524.292\n",
      "step 6100, train accuracy 0.668, train loss 547.596\n",
      "step 6200, train accuracy 0.712, train loss 463.509\n",
      "step 6300, train accuracy 0.676, train loss 540.059\n",
      "step 6400, train accuracy 0.652, train loss 525.185\n",
      "step 6500, train accuracy 0.684, train loss 481.682\n",
      "step 6600, train accuracy 0.628, train loss 524.78\n",
      "step 6700, train accuracy 0.648, train loss 508.538\n",
      "step 6800, train accuracy 0.666, train loss 497.277\n",
      "step 6900, train accuracy 0.682, train loss 459.416\n",
      "step 7000, train accuracy 0.714, train loss 409.724\n",
      "step 7100, train accuracy 0.648, train loss 520.018\n",
      "step 7200, train accuracy 0.656, train loss 492.688\n",
      "step 7300, train accuracy 0.658, train loss 473.916\n",
      "step 7400, train accuracy 0.712, train loss 479.995\n",
      "step 7500, train accuracy 0.672, train loss 457.736\n",
      "step 7600, train accuracy 0.676, train loss 495.497\n",
      "step 7700, train accuracy 0.666, train loss 511.207\n",
      "step 7800, train accuracy 0.71, train loss 454.03\n",
      "step 7900, train accuracy 0.708, train loss 467.955\n",
      "step 8000, train accuracy 0.688, train loss 480.926\n",
      "step 8100, train accuracy 0.714, train loss 427.976\n",
      "step 8200, train accuracy 0.712, train loss 430.755\n",
      "step 8300, train accuracy 0.686, train loss 477.288\n",
      "step 8400, train accuracy 0.698, train loss 433.337\n",
      "step 8500, train accuracy 0.75, train loss 421.52\n",
      "step 8600, train accuracy 0.748, train loss 409.883\n",
      "step 8700, train accuracy 0.696, train loss 466.296\n",
      "step 8800, train accuracy 0.672, train loss 484.816\n",
      "step 8900, train accuracy 0.696, train loss 424.478\n",
      "step 9000, train accuracy 0.762, train loss 409.105\n",
      "step 9100, train accuracy 0.738, train loss 402.757\n",
      "step 9200, train accuracy 0.752, train loss 386.579\n",
      "step 9300, train accuracy 0.73, train loss 410.895\n",
      "step 9400, train accuracy 0.712, train loss 445.246\n",
      "step 9500, train accuracy 0.754, train loss 354.445\n",
      "step 9600, train accuracy 0.758, train loss 399.112\n",
      "step 9700, train accuracy 0.754, train loss 432.72\n",
      "step 9800, train accuracy 0.77, train loss 384.094\n",
      "step 9900, train accuracy 0.714, train loss 432.501\n",
      "step 10000, train accuracy 0.774, train loss 333.855\n",
      "step 10100, train accuracy 0.76, train loss 373.877\n",
      "step 10200, train accuracy 0.738, train loss 390.579\n",
      "step 10300, train accuracy 0.71, train loss 406.137\n",
      "step 10400, train accuracy 0.782, train loss 332.137\n",
      "step 10500, train accuracy 0.746, train loss 380.224\n",
      "step 10600, train accuracy 0.746, train loss 371.705\n",
      "step 10700, train accuracy 0.746, train loss 410.352\n",
      "step 10800, train accuracy 0.762, train loss 368.962\n",
      "step 10900, train accuracy 0.77, train loss 350.239\n",
      "step 11000, train accuracy 0.752, train loss 366.258\n",
      "step 11100, train accuracy 0.77, train loss 389.175\n",
      "step 11200, train accuracy 0.754, train loss 400.376\n",
      "step 11300, train accuracy 0.754, train loss 362.24\n",
      "step 11400, train accuracy 0.764, train loss 374.128\n",
      "step 11500, train accuracy 0.774, train loss 353.828\n",
      "step 11600, train accuracy 0.75, train loss 361.831\n",
      "step 11700, train accuracy 0.76, train loss 372.193\n",
      "step 11800, train accuracy 0.766, train loss 386.25\n",
      "step 11900, train accuracy 0.768, train loss 364.827\n",
      "step 12000, train accuracy 0.776, train loss 325.248\n",
      "step 12100, train accuracy 0.792, train loss 387.792\n",
      "step 12200, train accuracy 0.8, train loss 332.1\n",
      "step 12300, train accuracy 0.794, train loss 309.015\n",
      "step 12400, train accuracy 0.812, train loss 281.192\n",
      "step 12500, train accuracy 0.762, train loss 357.962\n",
      "step 12600, train accuracy 0.774, train loss 344.61\n",
      "step 12700, train accuracy 0.778, train loss 334.346\n",
      "step 12800, train accuracy 0.808, train loss 324.819\n",
      "step 12900, train accuracy 0.804, train loss 295.141\n",
      "step 13000, train accuracy 0.784, train loss 331.665\n",
      "step 13100, train accuracy 0.81, train loss 350.615\n",
      "step 13200, train accuracy 0.802, train loss 324.775\n",
      "step 13300, train accuracy 0.798, train loss 317.388\n",
      "step 13400, train accuracy 0.806, train loss 307.952\n",
      "step 13500, train accuracy 0.798, train loss 294.984\n",
      "step 13600, train accuracy 0.8, train loss 356.225\n",
      "step 13700, train accuracy 0.814, train loss 292.508\n",
      "step 13800, train accuracy 0.794, train loss 317.643\n",
      "step 13900, train accuracy 0.808, train loss 310.746\n",
      "step 14000, train accuracy 0.826, train loss 283.161\n",
      "step 14100, train accuracy 0.806, train loss 302.065\n",
      "step 14200, train accuracy 0.806, train loss 284.621\n",
      "step 14300, train accuracy 0.838, train loss 258.002\n",
      "step 14400, train accuracy 0.852, train loss 264.762\n",
      "step 14500, train accuracy 0.812, train loss 297.639\n",
      "step 14600, train accuracy 0.842, train loss 261.848\n",
      "step 14700, train accuracy 0.822, train loss 292.825\n",
      "step 14800, train accuracy 0.844, train loss 265.696\n",
      "step 14900, train accuracy 0.83, train loss 282.11\n",
      "step 15000, train accuracy 0.85, train loss 280.627\n",
      "step 15100, train accuracy 0.826, train loss 322.753\n",
      "step 15200, train accuracy 0.838, train loss 271.734\n",
      "step 15300, train accuracy 0.806, train loss 300.938\n",
      "step 15400, train accuracy 0.848, train loss 237.38\n",
      "step 15500, train accuracy 0.828, train loss 290.087\n",
      "step 15600, train accuracy 0.84, train loss 260.723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15700, train accuracy 0.806, train loss 310.581\n",
      "step 15800, train accuracy 0.878, train loss 221.977\n",
      "step 15900, train accuracy 0.862, train loss 257.945\n",
      "step 16000, train accuracy 0.85, train loss 242.874\n",
      "step 16100, train accuracy 0.856, train loss 253.965\n",
      "step 16200, train accuracy 0.86, train loss 233.768\n",
      "step 16300, train accuracy 0.846, train loss 244.443\n",
      "step 16400, train accuracy 0.856, train loss 230.658\n",
      "step 16500, train accuracy 0.858, train loss 245.215\n",
      "step 16600, train accuracy 0.86, train loss 229.776\n",
      "step 16700, train accuracy 0.862, train loss 222.163\n",
      "step 16800, train accuracy 0.838, train loss 243.004\n",
      "step 16900, train accuracy 0.84, train loss 302.211\n",
      "step 17000, train accuracy 0.872, train loss 228.795\n",
      "step 17100, train accuracy 0.87, train loss 204.541\n",
      "step 17200, train accuracy 0.886, train loss 228.265\n",
      "step 17300, train accuracy 0.872, train loss 231.701\n",
      "step 17400, train accuracy 0.876, train loss 210.143\n",
      "step 17500, train accuracy 0.896, train loss 180.002\n",
      "step 17600, train accuracy 0.866, train loss 252.501\n",
      "step 17700, train accuracy 0.882, train loss 235.986\n",
      "step 17800, train accuracy 0.86, train loss 247.477\n",
      "step 17900, train accuracy 0.87, train loss 211.346\n",
      "step 18000, train accuracy 0.884, train loss 203.24\n",
      "step 18100, train accuracy 0.888, train loss 225.571\n",
      "step 18200, train accuracy 0.904, train loss 171.365\n",
      "step 18300, train accuracy 0.886, train loss 191.5\n",
      "step 18400, train accuracy 0.876, train loss 211.539\n",
      "step 18500, train accuracy 0.876, train loss 202.515\n",
      "step 18600, train accuracy 0.868, train loss 200.71\n",
      "step 18700, train accuracy 0.904, train loss 189.407\n",
      "step 18800, train accuracy 0.91, train loss 159.854\n",
      "step 18900, train accuracy 0.874, train loss 226.901\n",
      "step 19000, train accuracy 0.906, train loss 187.114\n",
      "step 19100, train accuracy 0.876, train loss 185.638\n",
      "step 19200, train accuracy 0.892, train loss 198.897\n",
      "step 19300, train accuracy 0.894, train loss 172.06\n",
      "step 19400, train accuracy 0.902, train loss 168.113\n",
      "step 19500, train accuracy 0.896, train loss 173.419\n",
      "step 19600, train accuracy 0.872, train loss 206.766\n",
      "step 19700, train accuracy 0.904, train loss 170.686\n",
      "step 19800, train accuracy 0.924, train loss 150.892\n",
      "step 19900, train accuracy 0.908, train loss 166.488\n",
      "step 20000, train accuracy 0.9, train loss 189.302\n",
      "step 20100, train accuracy 0.888, train loss 200.08\n",
      "step 20200, train accuracy 0.892, train loss 191.77\n",
      "step 20300, train accuracy 0.894, train loss 186.578\n",
      "step 20400, train accuracy 0.908, train loss 164.269\n",
      "step 20500, train accuracy 0.884, train loss 200.606\n",
      "step 20600, train accuracy 0.904, train loss 152.414\n",
      "step 20700, train accuracy 0.9, train loss 184.744\n",
      "step 20800, train accuracy 0.9, train loss 212.417\n",
      "step 20900, train accuracy 0.908, train loss 152.192\n",
      "step 21000, train accuracy 0.932, train loss 156.309\n",
      "step 21100, train accuracy 0.92, train loss 145.805\n",
      "step 21200, train accuracy 0.922, train loss 127.349\n",
      "step 21300, train accuracy 0.908, train loss 168.969\n",
      "step 21400, train accuracy 0.924, train loss 173.155\n",
      "step 21500, train accuracy 0.89, train loss 187.585\n",
      "step 21600, train accuracy 0.896, train loss 160.334\n",
      "step 21700, train accuracy 0.9, train loss 151.369\n",
      "step 21800, train accuracy 0.932, train loss 157.984\n",
      "step 21900, train accuracy 0.93, train loss 130.561\n",
      "step 22000, train accuracy 0.93, train loss 145.204\n",
      "step 22100, train accuracy 0.914, train loss 169.165\n",
      "step 22200, train accuracy 0.91, train loss 182.044\n",
      "step 22300, train accuracy 0.912, train loss 143.578\n",
      "step 22400, train accuracy 0.938, train loss 126.98\n",
      "step 22500, train accuracy 0.92, train loss 150.25\n",
      "step 22600, train accuracy 0.912, train loss 199.45\n",
      "step 22700, train accuracy 0.916, train loss 189.087\n",
      "step 22800, train accuracy 0.942, train loss 128.099\n",
      "step 22900, train accuracy 0.922, train loss 155.096\n",
      "step 23000, train accuracy 0.946, train loss 106.873\n",
      "step 23100, train accuracy 0.948, train loss 114.758\n",
      "step 23200, train accuracy 0.916, train loss 159.746\n",
      "step 23300, train accuracy 0.932, train loss 147.79\n",
      "step 23400, train accuracy 0.956, train loss 114.684\n",
      "step 23500, train accuracy 0.924, train loss 159.413\n",
      "step 23600, train accuracy 0.932, train loss 134.761\n",
      "step 23700, train accuracy 0.95, train loss 109.858\n",
      "step 23800, train accuracy 0.94, train loss 135.616\n",
      "step 23900, train accuracy 0.958, train loss 100.007\n",
      "step 24000, train accuracy 0.94, train loss 143.589\n",
      "step 24100, train accuracy 0.948, train loss 107.606\n",
      "step 24200, train accuracy 0.938, train loss 173.976\n",
      "step 24300, train accuracy 0.936, train loss 115.168\n",
      "step 24400, train accuracy 0.954, train loss 125.367\n",
      "step 24500, train accuracy 0.96, train loss 104.368\n",
      "step 24600, train accuracy 0.944, train loss 112.422\n",
      "step 24700, train accuracy 0.944, train loss 133.61\n",
      "step 24800, train accuracy 0.944, train loss 123.207\n",
      "step 24900, train accuracy 0.944, train loss 127.346\n",
      "step 25000, train accuracy 0.95, train loss 118.339\n",
      "step 25100, train accuracy 0.936, train loss 135.687\n",
      "step 25200, train accuracy 0.956, train loss 109.3\n",
      "step 25300, train accuracy 0.952, train loss 97.7966\n",
      "step 25400, train accuracy 0.95, train loss 114.997\n",
      "step 25500, train accuracy 0.958, train loss 100.869\n",
      "step 25600, train accuracy 0.934, train loss 136.488\n",
      "step 25700, train accuracy 0.946, train loss 109.463\n",
      "step 25800, train accuracy 0.934, train loss 116.396\n",
      "step 25900, train accuracy 0.96, train loss 100.154\n",
      "step 26000, train accuracy 0.982, train loss 68.0401\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data_shuffle.npy'\n",
    "vali_filename = 'E:/Alibaba German AI Challenge/data_process/sample_of_training.npy'\n",
    "data = np.load(filename)\n",
    "vali = np.load(vali_filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "print('The shape of vali is ',vali.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# flatten\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "#Eval = []\n",
    "\n",
    "batch_size = 500\n",
    "for i in range(300000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    temp_loss = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "#    vali_accuracy = accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "    Loss.append(temp_loss)\n",
    "#    Eval.append(vali_accuracy)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %g\" %(i, train_accuracy, temp_loss))\n",
    "        if train_accuracy > 0.98:\n",
    "            break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60820001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.75      0.39        71\n",
      "          1       0.56      0.41      0.47       342\n",
      "          2       0.72      0.31      0.43       474\n",
      "          3       0.34      0.64      0.44       119\n",
      "          4       0.36      0.49      0.42       217\n",
      "          5       0.58      0.47      0.52       487\n",
      "          6       0.33      0.93      0.49        40\n",
      "          7       0.77      0.50      0.60       586\n",
      "          8       0.43      0.62      0.51       188\n",
      "          9       0.28      0.48      0.35       158\n",
      "         10       0.95      0.80      0.87       622\n",
      "         11       0.26      0.86      0.39       125\n",
      "         12       0.50      0.69      0.58       134\n",
      "         13       0.83      0.45      0.58       585\n",
      "         14       0.35      1.00      0.52        26\n",
      "         15       0.50      0.86      0.63        91\n",
      "         16       0.98      0.96      0.97       735\n",
      "\n",
      "avg / total       0.70      0.61      0.62      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = y_conv.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})\n",
    "train_val_y = np.argmax(vali[:,-17:],axis = 1)\n",
    "pred_y = np.argmax(pred, axis = 1)\n",
    "print (classification_report(train_val_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngIuIiKQgBbiIiEgKUoCLiIikoP8PiX1e0z+MYjIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2128534f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
