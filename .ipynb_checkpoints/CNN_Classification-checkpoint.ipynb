{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352366"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/training.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "\n",
    "s1 = f['sen1']\n",
    "s2 = f['sen2']\n",
    "y = f['label']\n",
    "\n",
    "\n",
    "len(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 利用Adam算法优化CNN的demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the h5 file\n",
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.065\n",
      "step 0, train loss 2437.964355\n",
      "step 200, train accuracy 0.495\n",
      "step 200, train loss 580.493103\n",
      "step 400, train accuracy 0.575\n",
      "step 400, train loss 405.055420\n",
      "step 600, train accuracy 0.535\n",
      "step 600, train loss 473.820374\n",
      "step 800, train accuracy 0.585\n",
      "step 800, train loss 340.032013\n",
      "step 1000, train accuracy 0.545\n",
      "step 1000, train loss 331.545166\n",
      "step 1200, train accuracy 0.56\n",
      "step 1200, train loss 286.053162\n",
      "step 1400, train accuracy 0.56\n",
      "step 1400, train loss 289.171539\n",
      "step 1600, train accuracy 0.59\n",
      "step 1600, train loss 267.962952\n",
      "step 1800, train accuracy 0.64\n",
      "step 1800, train loss 252.462830\n",
      "step 2000, train accuracy 0.605\n",
      "step 2000, train loss 254.557953\n",
      "step 2200, train accuracy 0.68\n",
      "step 2200, train loss 235.472382\n",
      "step 2400, train accuracy 0.635\n",
      "step 2400, train loss 277.777618\n",
      "step 2600, train accuracy 0.68\n",
      "step 2600, train loss 206.952515\n",
      "step 2800, train accuracy 0.765\n",
      "step 2800, train loss 166.352997\n",
      "step 3000, train accuracy 0.725\n",
      "step 3000, train loss 215.458115\n",
      "step 3200, train accuracy 0.75\n",
      "step 3200, train loss 167.648071\n",
      "step 3400, train accuracy 0.73\n",
      "step 3400, train loss 210.244812\n",
      "step 3600, train accuracy 0.715\n",
      "step 3600, train loss 227.994995\n",
      "step 3800, train accuracy 0.745\n",
      "step 3800, train loss 154.620209\n",
      "step 4000, train accuracy 0.78\n",
      "step 4000, train loss 174.861465\n",
      "step 4200, train accuracy 0.77\n",
      "step 4200, train loss 158.974319\n",
      "step 4400, train accuracy 0.73\n",
      "step 4400, train loss 181.493347\n",
      "step 4600, train accuracy 0.77\n",
      "step 4600, train loss 138.827545\n",
      "step 4800, train accuracy 0.78\n",
      "step 4800, train loss 151.293213\n",
      "step 5000, train accuracy 0.81\n",
      "step 5000, train loss 142.529465\n",
      "step 5200, train accuracy 0.8\n",
      "step 5200, train loss 133.919724\n",
      "step 5400, train accuracy 0.77\n",
      "step 5400, train loss 130.379868\n",
      "step 5600, train accuracy 0.78\n",
      "step 5600, train loss 125.510117\n",
      "step 5800, train accuracy 0.805\n",
      "step 5800, train loss 118.152901\n",
      "step 6000, train accuracy 0.84\n",
      "step 6000, train loss 92.954239\n",
      "step 6200, train accuracy 0.82\n",
      "step 6200, train loss 103.942146\n",
      "step 6400, train accuracy 0.81\n",
      "step 6400, train loss 115.445686\n",
      "step 6600, train accuracy 0.84\n",
      "step 6600, train loss 94.019966\n",
      "step 6800, train accuracy 0.85\n",
      "step 6800, train loss 100.845596\n",
      "step 7000, train accuracy 0.87\n",
      "step 7000, train loss 100.869835\n",
      "step 7200, train accuracy 0.835\n",
      "step 7200, train loss 127.120247\n",
      "step 7400, train accuracy 0.865\n",
      "step 7400, train loss 102.194839\n",
      "step 7600, train accuracy 0.835\n",
      "step 7600, train loss 101.051819\n",
      "step 7800, train accuracy 0.895\n",
      "step 7800, train loss 96.531540\n",
      "step 8000, train accuracy 0.83\n",
      "step 8000, train loss 120.101547\n",
      "step 8200, train accuracy 0.855\n",
      "step 8200, train loss 112.852142\n",
      "step 8400, train accuracy 0.86\n",
      "step 8400, train loss 107.699928\n",
      "step 8600, train accuracy 0.89\n",
      "step 8600, train loss 75.519600\n",
      "step 8800, train accuracy 0.88\n",
      "step 8800, train loss 85.055359\n",
      "step 9000, train accuracy 0.895\n",
      "step 9000, train loss 75.078201\n",
      "step 9200, train accuracy 0.865\n",
      "step 9200, train loss 102.907066\n",
      "step 9400, train accuracy 0.9\n",
      "step 9400, train loss 75.814880\n",
      "step 9600, train accuracy 0.89\n",
      "step 9600, train loss 73.934776\n",
      "step 9800, train accuracy 0.895\n",
      "step 9800, train loss 89.417419\n",
      "step 10000, train accuracy 0.915\n",
      "step 10000, train loss 58.282734\n",
      "step 10200, train accuracy 0.91\n",
      "step 10200, train loss 66.719971\n",
      "step 10400, train accuracy 0.9\n",
      "step 10400, train loss 95.194382\n",
      "step 10600, train accuracy 0.94\n",
      "step 10600, train loss 49.280144\n",
      "step 10800, train accuracy 0.925\n",
      "step 10800, train loss 62.932861\n",
      "step 11000, train accuracy 0.895\n",
      "step 11000, train loss 53.299797\n",
      "step 11200, train accuracy 0.955\n",
      "step 11200, train loss 34.895920\n",
      "step 11400, train accuracy 0.935\n",
      "step 11400, train loss 52.923321\n",
      "step 11600, train accuracy 0.96\n",
      "step 11600, train loss 50.517723\n",
      "step 11800, train accuracy 0.915\n",
      "step 11800, train loss 60.261608\n",
      "step 12000, train accuracy 0.935\n",
      "step 12000, train loss 64.805695\n",
      "step 12200, train accuracy 0.965\n",
      "step 12200, train loss 40.216560\n",
      "step 12400, train accuracy 0.945\n",
      "step 12400, train loss 33.015030\n",
      "step 12600, train accuracy 0.95\n",
      "step 12600, train loss 82.451935\n",
      "step 12800, train accuracy 0.955\n",
      "step 12800, train loss 34.874657\n",
      "step 13000, train accuracy 0.955\n",
      "step 13000, train loss 48.321507\n",
      "step 13200, train accuracy 0.965\n",
      "step 13200, train loss 30.959270\n",
      "step 13400, train accuracy 0.97\n",
      "step 13400, train loss 45.528984\n",
      "step 13600, train accuracy 0.98\n",
      "step 13600, train loss 25.535673\n",
      "step 13800, train accuracy 0.995\n",
      "step 13800, train loss 20.482332\n",
      "step 14000, train accuracy 0.985\n",
      "step 14000, train loss 18.466410\n",
      "step 14200, train accuracy 0.98\n",
      "step 14200, train loss 26.327333\n",
      "step 14400, train accuracy 0.955\n",
      "step 14400, train loss 30.249176\n",
      "step 14600, train accuracy 0.995\n",
      "step 14600, train loss 16.551863\n",
      "step 14800, train accuracy 0.98\n",
      "step 14800, train loss 22.686859\n",
      "step 15000, train accuracy 0.985\n",
      "step 15000, train loss 19.294521\n",
      "step 15200, train accuracy 0.975\n",
      "step 15200, train loss 24.053753\n",
      "step 15400, train accuracy 0.975\n",
      "step 15400, train loss 34.955353\n",
      "step 15600, train accuracy 0.98\n",
      "step 15600, train loss 22.580660\n",
      "step 15800, train accuracy 0.98\n",
      "step 15800, train loss 32.997490\n",
      "step 16000, train accuracy 0.99\n",
      "step 16000, train loss 29.365328\n",
      "step 16200, train accuracy 0.975\n",
      "step 16200, train loss 16.401365\n",
      "step 16400, train accuracy 0.99\n",
      "step 16400, train loss 16.293262\n",
      "step 16600, train accuracy 0.995\n",
      "step 16600, train loss 12.770994\n",
      "step 16800, train accuracy 0.97\n",
      "step 16800, train loss 15.943648\n",
      "step 17000, train accuracy 0.985\n",
      "step 17000, train loss 14.073174\n",
      "step 17200, train accuracy 0.985\n",
      "step 17200, train loss 14.849368\n",
      "step 17400, train accuracy 0.98\n",
      "step 17400, train loss 16.124411\n",
      "step 17600, train accuracy 0.995\n",
      "step 17600, train loss 12.724537\n",
      "step 17800, train accuracy 0.995\n",
      "step 17800, train loss 12.508526\n",
      "step 18000, train accuracy 0.995\n",
      "step 18000, train loss 10.007954\n",
      "step 18200, train accuracy 1\n",
      "step 18200, train loss 7.450842\n",
      "step 18400, train accuracy 0.99\n",
      "step 18400, train loss 26.539940\n",
      "step 18600, train accuracy 1\n",
      "step 18600, train loss 8.261658\n",
      "step 18800, train accuracy 0.99\n",
      "step 18800, train loss 10.419234\n",
      "step 19000, train accuracy 0.995\n",
      "step 19000, train loss 9.230602\n",
      "step 19200, train accuracy 0.995\n",
      "step 19200, train loss 24.897961\n",
      "step 19400, train accuracy 1\n",
      "step 19400, train loss 4.769103\n",
      "step 19600, train accuracy 0.995\n",
      "step 19600, train loss 7.233519\n",
      "step 19800, train accuracy 0.995\n",
      "step 19800, train loss 8.534650\n",
      "Final:step 19892, train loss 3.613047\n",
      "Final accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/validation.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "print('Get the h5 file')\n",
    "\n",
    "s1 = np.array(f['sen1'])\n",
    "s2 = np.array(f['sen2'])\n",
    "y = np.array(f['label'])\n",
    "\n",
    "x = []\n",
    "for i in range(0,s1.shape[0]):\n",
    "    temp1 = s1[i].flatten()\n",
    "    temp2 = s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    x.append(temp)\n",
    "x = np.array(x)\n",
    "\n",
    "data = np.hstack((x,y))\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(30000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "        print('step %d, train loss %f'%(i,loss_temp))\n",
    "    if loss_temp < 4:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "        print (\"Final accuracy %g\" %train_accuracy)\n",
    "        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEfEhBbiIiIgPKcBFRER86P8DkNQRqVH0s3YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1c518b6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('second_20k_vali_as_train_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 使用均衡分布的数据进行训练 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "step 0, train accuracy 0.095\n",
      "step 0, train loss 2501.947998\n",
      "step 250, train accuracy 0.325\n",
      "step 250, train loss 691.175171\n",
      "step 500, train accuracy 0.4\n",
      "step 500, train loss 464.166840\n",
      "step 750, train accuracy 0.385\n",
      "step 750, train loss 478.504944\n",
      "step 1000, train accuracy 0.415\n",
      "step 1000, train loss 383.896942\n",
      "step 1250, train accuracy 0.365\n",
      "step 1250, train loss 419.449768\n",
      "step 1500, train accuracy 0.425\n",
      "step 1500, train loss 420.824310\n",
      "step 1750, train accuracy 0.43\n",
      "step 1750, train loss 395.652863\n",
      "step 2000, train accuracy 0.43\n",
      "step 2000, train loss 363.959473\n",
      "step 2250, train accuracy 0.42\n",
      "step 2250, train loss 341.458374\n",
      "step 2500, train accuracy 0.555\n",
      "step 2500, train loss 306.630798\n",
      "step 2750, train accuracy 0.475\n",
      "step 2750, train loss 323.587952\n",
      "step 3000, train accuracy 0.48\n",
      "step 3000, train loss 302.360168\n",
      "step 3250, train accuracy 0.475\n",
      "step 3250, train loss 299.143127\n",
      "step 3500, train accuracy 0.495\n",
      "step 3500, train loss 300.170990\n",
      "step 3750, train accuracy 0.51\n",
      "step 3750, train loss 311.753876\n",
      "step 4000, train accuracy 0.59\n",
      "step 4000, train loss 258.111206\n",
      "step 4250, train accuracy 0.55\n",
      "step 4250, train loss 258.508026\n",
      "step 4500, train accuracy 0.5\n",
      "step 4500, train loss 311.390320\n",
      "step 4750, train accuracy 0.48\n",
      "step 4750, train loss 328.088348\n",
      "step 5000, train accuracy 0.475\n",
      "step 5000, train loss 316.329529\n",
      "step 5250, train accuracy 0.57\n",
      "step 5250, train loss 263.346313\n",
      "step 5500, train accuracy 0.515\n",
      "step 5500, train loss 284.621368\n",
      "step 5750, train accuracy 0.59\n",
      "step 5750, train loss 229.452347\n",
      "step 6000, train accuracy 0.595\n",
      "step 6000, train loss 238.978439\n",
      "step 6250, train accuracy 0.585\n",
      "step 6250, train loss 250.040436\n",
      "step 6500, train accuracy 0.565\n",
      "step 6500, train loss 257.015137\n",
      "step 6750, train accuracy 0.505\n",
      "step 6750, train loss 269.617218\n",
      "step 7000, train accuracy 0.64\n",
      "step 7000, train loss 213.652893\n",
      "step 7250, train accuracy 0.56\n",
      "step 7250, train loss 272.556641\n",
      "step 7500, train accuracy 0.65\n",
      "step 7500, train loss 217.420715\n",
      "step 7750, train accuracy 0.6\n",
      "step 7750, train loss 245.837555\n",
      "step 8000, train accuracy 0.6\n",
      "step 8000, train loss 240.630707\n",
      "step 8250, train accuracy 0.63\n",
      "step 8250, train loss 229.314178\n",
      "step 8500, train accuracy 0.56\n",
      "step 8500, train loss 258.536011\n",
      "step 8750, train accuracy 0.585\n",
      "step 8750, train loss 231.353897\n",
      "step 9000, train accuracy 0.595\n",
      "step 9000, train loss 239.685043\n",
      "step 9250, train accuracy 0.615\n",
      "step 9250, train loss 219.584000\n",
      "step 9500, train accuracy 0.565\n",
      "step 9500, train loss 231.586411\n",
      "step 9750, train accuracy 0.59\n",
      "step 9750, train loss 236.071762\n",
      "step 10000, train accuracy 0.585\n",
      "step 10000, train loss 240.872635\n",
      "step 10250, train accuracy 0.62\n",
      "step 10250, train loss 220.365372\n",
      "step 10500, train accuracy 0.63\n",
      "step 10500, train loss 200.533051\n",
      "step 10750, train accuracy 0.66\n",
      "step 10750, train loss 215.022614\n",
      "step 11000, train accuracy 0.68\n",
      "step 11000, train loss 210.721954\n",
      "step 11250, train accuracy 0.61\n",
      "step 11250, train loss 231.508301\n",
      "step 11500, train accuracy 0.595\n",
      "step 11500, train loss 235.371429\n",
      "step 11750, train accuracy 0.61\n",
      "step 11750, train loss 222.915359\n",
      "step 12000, train accuracy 0.645\n",
      "step 12000, train loss 198.152939\n",
      "step 12250, train accuracy 0.63\n",
      "step 12250, train loss 218.070084\n",
      "step 12500, train accuracy 0.675\n",
      "step 12500, train loss 184.892258\n",
      "step 12750, train accuracy 0.66\n",
      "step 12750, train loss 196.683670\n",
      "step 13000, train accuracy 0.635\n",
      "step 13000, train loss 203.013260\n",
      "step 13250, train accuracy 0.61\n",
      "step 13250, train loss 242.405365\n",
      "step 13500, train accuracy 0.68\n",
      "step 13500, train loss 190.536392\n",
      "step 13750, train accuracy 0.65\n",
      "step 13750, train loss 203.206253\n",
      "step 14000, train accuracy 0.67\n",
      "step 14000, train loss 250.225754\n",
      "step 14250, train accuracy 0.67\n",
      "step 14250, train loss 206.796890\n",
      "step 14500, train accuracy 0.7\n",
      "step 14500, train loss 181.381424\n",
      "step 14750, train accuracy 0.655\n",
      "step 14750, train loss 187.097839\n",
      "step 15000, train accuracy 0.69\n",
      "step 15000, train loss 190.400787\n",
      "step 15250, train accuracy 0.675\n",
      "step 15250, train loss 187.558350\n",
      "step 15500, train accuracy 0.61\n",
      "step 15500, train loss 220.053101\n",
      "step 15750, train accuracy 0.71\n",
      "step 15750, train loss 193.496216\n",
      "step 16000, train accuracy 0.67\n",
      "step 16000, train loss 204.453339\n",
      "step 16250, train accuracy 0.705\n",
      "step 16250, train loss 179.162979\n",
      "step 16500, train accuracy 0.735\n",
      "step 16500, train loss 168.343079\n",
      "step 16750, train accuracy 0.635\n",
      "step 16750, train loss 197.536606\n",
      "step 17000, train accuracy 0.715\n",
      "step 17000, train loss 162.504150\n",
      "step 17250, train accuracy 0.715\n",
      "step 17250, train loss 159.959137\n",
      "step 17500, train accuracy 0.69\n",
      "step 17500, train loss 157.129547\n",
      "step 17750, train accuracy 0.73\n",
      "step 17750, train loss 169.675552\n",
      "step 18000, train accuracy 0.7\n",
      "step 18000, train loss 168.713043\n",
      "step 18250, train accuracy 0.73\n",
      "step 18250, train loss 174.658768\n",
      "step 18500, train accuracy 0.725\n",
      "step 18500, train loss 169.415283\n",
      "step 18750, train accuracy 0.73\n",
      "step 18750, train loss 153.456467\n",
      "step 19000, train accuracy 0.685\n",
      "step 19000, train loss 179.562469\n",
      "step 19250, train accuracy 0.675\n",
      "step 19250, train loss 181.403244\n",
      "step 19500, train accuracy 0.655\n",
      "step 19500, train loss 184.268097\n",
      "step 19750, train accuracy 0.725\n",
      "step 19750, train loss 153.515213\n",
      "step 20000, train accuracy 0.745\n",
      "step 20000, train loss 145.189728\n",
      "step 20250, train accuracy 0.7\n",
      "step 20250, train loss 170.604141\n",
      "step 20500, train accuracy 0.79\n",
      "step 20500, train loss 151.715485\n",
      "step 20750, train accuracy 0.79\n",
      "step 20750, train loss 144.890976\n",
      "step 21000, train accuracy 0.78\n",
      "step 21000, train loss 144.851303\n",
      "step 21250, train accuracy 0.715\n",
      "step 21250, train loss 156.311829\n",
      "step 21500, train accuracy 0.755\n",
      "step 21500, train loss 154.785919\n",
      "step 21750, train accuracy 0.765\n",
      "step 21750, train loss 148.689774\n",
      "step 22000, train accuracy 0.705\n",
      "step 22000, train loss 170.298508\n",
      "step 22250, train accuracy 0.77\n",
      "step 22250, train loss 158.865494\n",
      "step 22500, train accuracy 0.72\n",
      "step 22500, train loss 171.397659\n",
      "step 22750, train accuracy 0.82\n",
      "step 22750, train loss 143.836853\n",
      "step 23000, train accuracy 0.78\n",
      "step 23000, train loss 156.815613\n",
      "step 23250, train accuracy 0.775\n",
      "step 23250, train loss 145.757339\n",
      "step 23500, train accuracy 0.715\n",
      "step 23500, train loss 192.925919\n",
      "step 23750, train accuracy 0.77\n",
      "step 23750, train loss 142.493912\n",
      "step 24000, train accuracy 0.735\n",
      "step 24000, train loss 146.738037\n",
      "step 24250, train accuracy 0.725\n",
      "step 24250, train loss 150.799713\n",
      "step 24500, train accuracy 0.78\n",
      "step 24500, train loss 134.493103\n",
      "step 24750, train accuracy 0.785\n",
      "step 24750, train loss 142.152481\n",
      "step 25000, train accuracy 0.72\n",
      "step 25000, train loss 169.016479\n",
      "step 25250, train accuracy 0.765\n",
      "step 25250, train loss 134.902222\n",
      "step 25500, train accuracy 0.8\n",
      "step 25500, train loss 152.161179\n",
      "step 25750, train accuracy 0.775\n",
      "step 25750, train loss 126.139786\n",
      "step 26000, train accuracy 0.765\n",
      "step 26000, train loss 138.020950\n",
      "step 26250, train accuracy 0.775\n",
      "step 26250, train loss 118.575531\n",
      "step 26500, train accuracy 0.695\n",
      "step 26500, train loss 148.555267\n",
      "step 26750, train accuracy 0.86\n",
      "step 26750, train loss 125.237694\n",
      "step 27000, train accuracy 0.845\n",
      "step 27000, train loss 108.605743\n",
      "step 27250, train accuracy 0.785\n",
      "step 27250, train loss 134.921539\n",
      "step 27500, train accuracy 0.825\n",
      "step 27500, train loss 133.037598\n",
      "step 27750, train accuracy 0.795\n",
      "step 27750, train loss 114.235733\n",
      "step 28000, train accuracy 0.8\n",
      "step 28000, train loss 116.423225\n",
      "step 28250, train accuracy 0.76\n",
      "step 28250, train loss 136.295212\n",
      "step 28500, train accuracy 0.805\n",
      "step 28500, train loss 115.910805\n",
      "step 28750, train accuracy 0.77\n",
      "step 28750, train loss 128.162140\n",
      "step 29000, train accuracy 0.8\n",
      "step 29000, train loss 135.548065\n",
      "step 29250, train accuracy 0.78\n",
      "step 29250, train loss 127.885857\n",
      "step 29500, train accuracy 0.735\n",
      "step 29500, train loss 138.035156\n",
      "step 29750, train accuracy 0.78\n",
      "step 29750, train loss 111.320900\n",
      "step 30000, train accuracy 0.78\n",
      "step 30000, train loss 128.282181\n",
      "step 30250, train accuracy 0.815\n",
      "step 30250, train loss 137.791840\n",
      "step 30500, train accuracy 0.81\n",
      "step 30500, train loss 105.711861\n",
      "step 30750, train accuracy 0.765\n",
      "step 30750, train loss 128.067657\n",
      "step 31000, train accuracy 0.8\n",
      "step 31000, train loss 120.111191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31250, train accuracy 0.84\n",
      "step 31250, train loss 114.688187\n",
      "step 31500, train accuracy 0.835\n",
      "step 31500, train loss 113.541542\n",
      "step 31750, train accuracy 0.805\n",
      "step 31750, train loss 141.345490\n",
      "step 32000, train accuracy 0.86\n",
      "step 32000, train loss 102.159286\n",
      "step 32250, train accuracy 0.775\n",
      "step 32250, train loss 120.237350\n",
      "step 32500, train accuracy 0.825\n",
      "step 32500, train loss 100.407204\n",
      "step 32750, train accuracy 0.825\n",
      "step 32750, train loss 110.903641\n",
      "step 33000, train accuracy 0.825\n",
      "step 33000, train loss 110.227005\n",
      "step 33250, train accuracy 0.855\n",
      "step 33250, train loss 101.350990\n",
      "step 33500, train accuracy 0.825\n",
      "step 33500, train loss 103.588120\n",
      "step 33750, train accuracy 0.825\n",
      "step 33750, train loss 102.337891\n",
      "step 34000, train accuracy 0.79\n",
      "step 34000, train loss 119.780388\n",
      "step 34250, train accuracy 0.855\n",
      "step 34250, train loss 114.606903\n",
      "step 34500, train accuracy 0.855\n",
      "step 34500, train loss 90.204430\n",
      "step 34750, train accuracy 0.82\n",
      "step 34750, train loss 125.883163\n",
      "step 35000, train accuracy 0.86\n",
      "step 35000, train loss 88.387428\n",
      "step 35250, train accuracy 0.835\n",
      "step 35250, train loss 102.264236\n",
      "step 35500, train accuracy 0.885\n",
      "step 35500, train loss 76.856850\n",
      "step 35750, train accuracy 0.81\n",
      "step 35750, train loss 99.735825\n",
      "step 36000, train accuracy 0.895\n",
      "step 36000, train loss 84.675262\n",
      "step 36250, train accuracy 0.84\n",
      "step 36250, train loss 90.167885\n",
      "step 36500, train accuracy 0.845\n",
      "step 36500, train loss 85.913544\n",
      "step 36750, train accuracy 0.865\n",
      "step 36750, train loss 81.286446\n",
      "step 37000, train accuracy 0.83\n",
      "step 37000, train loss 96.534187\n",
      "step 37250, train accuracy 0.875\n",
      "step 37250, train loss 83.940994\n",
      "step 37500, train accuracy 0.86\n",
      "step 37500, train loss 92.176338\n",
      "step 37750, train accuracy 0.89\n",
      "step 37750, train loss 80.160965\n",
      "step 38000, train accuracy 0.885\n",
      "step 38000, train loss 69.371712\n",
      "step 38250, train accuracy 0.875\n",
      "step 38250, train loss 77.216469\n",
      "step 38500, train accuracy 0.835\n",
      "step 38500, train loss 89.416954\n",
      "step 38750, train accuracy 0.83\n",
      "step 38750, train loss 83.469086\n",
      "step 39000, train accuracy 0.86\n",
      "step 39000, train loss 77.792397\n",
      "step 39250, train accuracy 0.875\n",
      "step 39250, train loss 81.356689\n",
      "step 39500, train accuracy 0.9\n",
      "step 39500, train loss 84.220703\n",
      "step 39750, train accuracy 0.89\n",
      "step 39750, train loss 70.232384\n",
      "step 40000, train accuracy 0.86\n",
      "step 40000, train loss 92.989769\n",
      "step 40250, train accuracy 0.825\n",
      "step 40250, train loss 110.280464\n",
      "step 40500, train accuracy 0.9\n",
      "step 40500, train loss 70.515137\n",
      "step 40750, train accuracy 0.895\n",
      "step 40750, train loss 61.057423\n",
      "step 41000, train accuracy 0.825\n",
      "step 41000, train loss 97.829414\n",
      "step 41250, train accuracy 0.875\n",
      "step 41250, train loss 71.082001\n",
      "step 41500, train accuracy 0.84\n",
      "step 41500, train loss 84.904709\n",
      "step 41750, train accuracy 0.87\n",
      "step 41750, train loss 77.212471\n",
      "step 42000, train accuracy 0.875\n",
      "step 42000, train loss 76.397324\n",
      "step 42250, train accuracy 0.865\n",
      "step 42250, train loss 99.975761\n",
      "step 42500, train accuracy 0.885\n",
      "step 42500, train loss 69.439934\n",
      "step 42750, train accuracy 0.92\n",
      "step 42750, train loss 59.634369\n",
      "step 43000, train accuracy 0.88\n",
      "step 43000, train loss 71.642212\n",
      "step 43250, train accuracy 0.89\n",
      "step 43250, train loss 67.396111\n",
      "step 43500, train accuracy 0.89\n",
      "step 43500, train loss 59.475021\n",
      "step 43750, train accuracy 0.905\n",
      "step 43750, train loss 55.938202\n",
      "step 44000, train accuracy 0.905\n",
      "step 44000, train loss 78.140038\n",
      "step 44250, train accuracy 0.87\n",
      "step 44250, train loss 93.323471\n",
      "step 44500, train accuracy 0.88\n",
      "step 44500, train loss 68.625183\n",
      "step 44750, train accuracy 0.905\n",
      "step 44750, train loss 58.031837\n",
      "step 45000, train accuracy 0.87\n",
      "step 45000, train loss 72.644783\n",
      "step 45250, train accuracy 0.895\n",
      "step 45250, train loss 75.343758\n",
      "step 45500, train accuracy 0.9\n",
      "step 45500, train loss 63.779221\n",
      "step 45750, train accuracy 0.91\n",
      "step 45750, train loss 69.708374\n",
      "step 46000, train accuracy 0.89\n",
      "step 46000, train loss 66.751625\n",
      "step 46250, train accuracy 0.91\n",
      "step 46250, train loss 60.384289\n",
      "step 46500, train accuracy 0.93\n",
      "step 46500, train loss 50.390560\n",
      "step 46750, train accuracy 0.925\n",
      "step 46750, train loss 46.527191\n",
      "step 47000, train accuracy 0.915\n",
      "step 47000, train loss 54.374443\n",
      "step 47250, train accuracy 0.905\n",
      "step 47250, train loss 54.791977\n",
      "step 47500, train accuracy 0.875\n",
      "step 47500, train loss 73.940178\n",
      "step 47750, train accuracy 0.93\n",
      "step 47750, train loss 45.551617\n",
      "step 48000, train accuracy 0.935\n",
      "step 48000, train loss 48.086292\n",
      "step 48250, train accuracy 0.95\n",
      "step 48250, train loss 62.570950\n",
      "step 48500, train accuracy 0.945\n",
      "step 48500, train loss 47.738892\n",
      "step 48750, train accuracy 0.94\n",
      "step 48750, train loss 56.709915\n",
      "step 49000, train accuracy 0.955\n",
      "step 49000, train loss 47.083828\n",
      "step 49250, train accuracy 0.94\n",
      "step 49250, train loss 52.414742\n",
      "step 49500, train accuracy 0.93\n",
      "step 49500, train loss 46.952011\n",
      "step 49750, train accuracy 0.965\n",
      "step 49750, train loss 40.506454\n",
      "step 50000, train accuracy 0.965\n",
      "step 50000, train loss 42.614887\n",
      "step 50250, train accuracy 0.945\n",
      "step 50250, train loss 43.654232\n",
      "step 50500, train accuracy 0.925\n",
      "step 50500, train loss 66.967773\n",
      "step 50750, train accuracy 0.96\n",
      "step 50750, train loss 35.573090\n",
      "step 51000, train accuracy 0.925\n",
      "step 51000, train loss 52.290459\n",
      "step 51250, train accuracy 0.95\n",
      "step 51250, train loss 41.574791\n",
      "step 51500, train accuracy 0.97\n",
      "step 51500, train loss 32.036385\n",
      "step 51750, train accuracy 0.945\n",
      "step 51750, train loss 37.481083\n",
      "step 52000, train accuracy 0.95\n",
      "step 52000, train loss 35.851803\n",
      "step 52250, train accuracy 0.97\n",
      "step 52250, train loss 36.114502\n",
      "step 52500, train accuracy 0.92\n",
      "step 52500, train loss 52.234749\n",
      "step 52750, train accuracy 0.94\n",
      "step 52750, train loss 39.714352\n",
      "step 53000, train accuracy 0.905\n",
      "step 53000, train loss 53.130585\n",
      "step 53250, train accuracy 0.93\n",
      "step 53250, train loss 44.409340\n",
      "step 53500, train accuracy 0.935\n",
      "step 53500, train loss 36.016239\n",
      "step 53750, train accuracy 0.965\n",
      "step 53750, train loss 46.709000\n",
      "step 54000, train accuracy 0.95\n",
      "step 54000, train loss 40.193993\n",
      "step 54250, train accuracy 0.945\n",
      "step 54250, train loss 32.962631\n",
      "step 54500, train accuracy 0.97\n",
      "step 54500, train loss 28.694590\n",
      "step 54750, train accuracy 0.975\n",
      "step 54750, train loss 23.381413\n",
      "step 55000, train accuracy 0.95\n",
      "step 55000, train loss 34.200321\n",
      "step 55250, train accuracy 0.975\n",
      "step 55250, train loss 26.345453\n",
      "step 55500, train accuracy 0.97\n",
      "step 55500, train loss 31.116522\n",
      "step 55750, train accuracy 0.96\n",
      "step 55750, train loss 30.382528\n",
      "step 56000, train accuracy 0.965\n",
      "step 56000, train loss 28.437944\n",
      "step 56250, train accuracy 0.97\n",
      "step 56250, train loss 29.218138\n",
      "step 56500, train accuracy 0.975\n",
      "step 56500, train loss 26.913303\n",
      "step 56750, train accuracy 0.96\n",
      "step 56750, train loss 33.170418\n",
      "step 57000, train accuracy 0.975\n",
      "step 57000, train loss 28.243034\n",
      "step 57250, train accuracy 0.965\n",
      "step 57250, train loss 28.855202\n",
      "step 57500, train accuracy 0.955\n",
      "step 57500, train loss 43.355358\n",
      "step 57750, train accuracy 0.965\n",
      "step 57750, train loss 30.643475\n",
      "step 58000, train accuracy 0.97\n",
      "step 58000, train loss 25.213587\n",
      "step 58250, train accuracy 0.97\n",
      "step 58250, train loss 23.910816\n",
      "step 58500, train accuracy 0.98\n",
      "step 58500, train loss 25.655304\n",
      "step 58750, train accuracy 0.97\n",
      "step 58750, train loss 27.872833\n",
      "step 59000, train accuracy 0.975\n",
      "step 59000, train loss 34.463711\n",
      "step 59250, train accuracy 0.95\n",
      "step 59250, train loss 33.610729\n",
      "step 59500, train accuracy 0.97\n",
      "step 59500, train loss 26.098040\n",
      "step 59750, train accuracy 0.97\n",
      "step 59750, train loss 21.768784\n",
      "step 60000, train accuracy 0.975\n",
      "step 60000, train loss 22.471706\n",
      "step 60250, train accuracy 0.995\n",
      "step 60250, train loss 23.230228\n",
      "step 60500, train accuracy 0.97\n",
      "step 60500, train loss 26.498486\n",
      "step 60750, train accuracy 0.96\n",
      "step 60750, train loss 24.435865\n",
      "step 61000, train accuracy 0.96\n",
      "step 61000, train loss 41.481770\n",
      "step 61250, train accuracy 0.975\n",
      "step 61250, train loss 18.157310\n",
      "step 61500, train accuracy 0.96\n",
      "step 61500, train loss 27.538250\n",
      "step 61750, train accuracy 0.98\n",
      "step 61750, train loss 33.555702\n",
      "step 62000, train accuracy 0.975\n",
      "step 62000, train loss 19.532280\n",
      "step 62250, train accuracy 0.97\n",
      "step 62250, train loss 23.154436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62500, train accuracy 0.98\n",
      "step 62500, train loss 17.834454\n",
      "step 62750, train accuracy 0.985\n",
      "step 62750, train loss 19.469364\n",
      "step 63000, train accuracy 0.99\n",
      "step 63000, train loss 13.726264\n",
      "step 63250, train accuracy 0.985\n",
      "step 63250, train loss 30.752413\n",
      "step 63500, train accuracy 0.985\n",
      "step 63500, train loss 29.016226\n",
      "step 63750, train accuracy 0.985\n",
      "step 63750, train loss 17.810101\n",
      "step 64000, train accuracy 0.99\n",
      "step 64000, train loss 18.153650\n",
      "step 64250, train accuracy 0.97\n",
      "step 64250, train loss 19.629835\n",
      "step 64500, train accuracy 0.985\n",
      "step 64500, train loss 23.171654\n",
      "step 64750, train accuracy 0.99\n",
      "step 64750, train loss 12.786263\n",
      "step 65000, train accuracy 0.99\n",
      "step 65000, train loss 13.052101\n",
      "step 65250, train accuracy 0.995\n",
      "step 65250, train loss 14.060141\n",
      "step 65500, train accuracy 0.985\n",
      "step 65500, train loss 28.558401\n",
      "step 65750, train accuracy 0.985\n",
      "step 65750, train loss 18.265156\n",
      "step 66000, train accuracy 0.975\n",
      "step 66000, train loss 24.421934\n",
      "step 66250, train accuracy 0.985\n",
      "step 66250, train loss 27.938122\n",
      "step 66500, train accuracy 0.99\n",
      "step 66500, train loss 16.418962\n",
      "step 66750, train accuracy 0.99\n",
      "step 66750, train loss 11.002378\n",
      "step 67000, train accuracy 0.985\n",
      "step 67000, train loss 15.439715\n",
      "step 67250, train accuracy 0.985\n",
      "step 67250, train loss 12.681532\n",
      "step 67500, train accuracy 0.985\n",
      "step 67500, train loss 11.526389\n",
      "step 67750, train accuracy 0.99\n",
      "step 67750, train loss 11.612343\n",
      "step 68000, train accuracy 0.975\n",
      "step 68000, train loss 17.595818\n",
      "step 68250, train accuracy 0.995\n",
      "step 68250, train loss 10.190494\n",
      "step 68500, train accuracy 0.985\n",
      "step 68500, train loss 13.916505\n",
      "step 68750, train accuracy 0.99\n",
      "step 68750, train loss 25.337795\n",
      "step 69000, train accuracy 0.99\n",
      "step 69000, train loss 13.491289\n",
      "step 69250, train accuracy 0.99\n",
      "step 69250, train loss 10.126125\n",
      "step 69500, train accuracy 0.99\n",
      "step 69500, train loss 12.464053\n",
      "step 69750, train accuracy 0.99\n",
      "step 69750, train loss 12.760547\n",
      "step 70000, train accuracy 0.985\n",
      "step 70000, train loss 13.724017\n",
      "step 70250, train accuracy 1\n",
      "step 70250, train loss 12.222582\n",
      "step 70500, train accuracy 1\n",
      "step 70500, train loss 6.598807\n",
      "step 70750, train accuracy 1\n",
      "step 70750, train loss 6.941884\n",
      "step 71000, train accuracy 1\n",
      "step 71000, train loss 7.305584\n",
      "step 71250, train accuracy 0.995\n",
      "step 71250, train loss 9.564557\n",
      "step 71500, train accuracy 1\n",
      "step 71500, train loss 8.472621\n",
      "step 71750, train accuracy 1\n",
      "step 71750, train loss 9.701490\n",
      "step 72000, train accuracy 0.995\n",
      "step 72000, train loss 8.948296\n",
      "step 72250, train accuracy 1\n",
      "step 72250, train loss 8.397684\n",
      "step 72500, train accuracy 1\n",
      "step 72500, train loss 7.662931\n",
      "Final:step 72626, train loss 3.942866\n",
      "Final accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data.npy'\n",
    "\n",
    "\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(100000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%250 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "        print('step %d, train loss %f'%(i,loss_temp))\n",
    "    if loss_temp < 4:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print('Final:step %d, train loss %f'%(i,loss_temp))\n",
    "        print (\"Final accuracy %g\" %train_accuracy)\n",
    "        break\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAE/CAYAAAC5EpGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSAj9f7cVvHcUUXS/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ee26f15320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('third_100k_batch_balance_50k_train_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
