{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352366"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/training.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "\n",
    "s1 = f['sen1']\n",
    "s2 = f['sen2']\n",
    "y = f['label']\n",
    "\n",
    "\n",
    "len(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the h5 file\n",
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.12\n",
      "step 200, train accuracy 0.355\n",
      "step 400, train accuracy 0.465\n",
      "step 600, train accuracy 0.475\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                           \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "def get_data(f):\n",
    "    s1 = np.array(f['sen1'])\n",
    "    s2 = np.array(f['sen2'])\n",
    "    y = np.array(f['label'])\n",
    "\n",
    "    x = []\n",
    "    for i in range(0,s1.shape[0]):\n",
    "        temp1 = s1[i].flatten()\n",
    "        temp2 = s2[i].flatten()\n",
    "        temp = np.hstack((temp1,temp2))\n",
    "        x.append(temp)\n",
    "    x = np.array(x)\n",
    "\n",
    "    data = np.hstack((x,y))\n",
    "    print('The shape of data is ',data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 18, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/validation.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "print('Get the h5 file')\n",
    "\n",
    "data = get_data(f)\n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "for i in range(640):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "\n",
    "#print (\"test accuracy %g\" % accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
