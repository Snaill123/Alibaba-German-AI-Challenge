{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (50989, 18449)\n",
      "step 0, train accuracy 0.102, train loss 12395.158203\n",
      "step 200, train accuracy 0.353, train loss 2521.895508\n",
      "step 400, train accuracy 0.458, train loss 2030.161621\n",
      "step 600, train accuracy 0.484, train loss 1900.529053\n",
      "step 800, train accuracy 0.48, train loss 1632.001221\n",
      "step 1000, train accuracy 0.513, train loss 1520.481079\n",
      "step 1200, train accuracy 0.532, train loss 1416.726196\n",
      "step 1400, train accuracy 0.565, train loss 1332.280273\n",
      "step 1600, train accuracy 0.578, train loss 1278.763794\n",
      "step 1800, train accuracy 0.572, train loss 1291.292969\n",
      "step 2000, train accuracy 0.599, train loss 1211.583374\n",
      "step 2200, train accuracy 0.605, train loss 1244.936523\n",
      "step 2400, train accuracy 0.624, train loss 1133.782471\n",
      "step 2600, train accuracy 0.631, train loss 1166.313965\n",
      "step 2800, train accuracy 0.642, train loss 1056.976807\n",
      "step 3000, train accuracy 0.631, train loss 1063.520630\n",
      "step 3200, train accuracy 0.657, train loss 1036.327515\n",
      "step 3400, train accuracy 0.691, train loss 986.379517\n",
      "step 3600, train accuracy 0.676, train loss 1006.033264\n",
      "step 3800, train accuracy 0.659, train loss 1030.382202\n",
      "step 4000, train accuracy 0.683, train loss 946.165283\n",
      "step 4200, train accuracy 0.685, train loss 920.647217\n",
      "step 4400, train accuracy 0.689, train loss 905.065125\n",
      "step 4600, train accuracy 0.706, train loss 902.189819\n",
      "step 4800, train accuracy 0.73, train loss 851.031250\n",
      "step 5000, train accuracy 0.716, train loss 836.598816\n",
      "step 5200, train accuracy 0.73, train loss 853.203491\n",
      "step 5400, train accuracy 0.741, train loss 803.455200\n",
      "step 5600, train accuracy 0.733, train loss 826.663391\n",
      "step 5800, train accuracy 0.741, train loss 791.124268\n",
      "step 6000, train accuracy 0.771, train loss 733.276978\n",
      "step 6200, train accuracy 0.748, train loss 746.330872\n",
      "step 6400, train accuracy 0.767, train loss 805.298645\n",
      "step 6600, train accuracy 0.779, train loss 683.383423\n",
      "step 6800, train accuracy 0.793, train loss 681.314941\n",
      "step 7000, train accuracy 0.791, train loss 654.354492\n",
      "step 7200, train accuracy 0.801, train loss 628.817566\n",
      "step 7400, train accuracy 0.8, train loss 626.086914\n",
      "step 7600, train accuracy 0.798, train loss 645.016052\n",
      "step 7800, train accuracy 0.828, train loss 583.987122\n",
      "step 8000, train accuracy 0.82, train loss 572.615601\n",
      "step 8200, train accuracy 0.805, train loss 595.147461\n",
      "step 8400, train accuracy 0.826, train loss 570.010681\n",
      "step 8600, train accuracy 0.807, train loss 589.307739\n",
      "step 8800, train accuracy 0.819, train loss 579.315674\n",
      "step 9000, train accuracy 0.823, train loss 556.074890\n",
      "step 9200, train accuracy 0.83, train loss 542.043274\n",
      "step 9400, train accuracy 0.866, train loss 483.541626\n",
      "step 9600, train accuracy 0.849, train loss 486.559143\n",
      "step 9800, train accuracy 0.855, train loss 503.309998\n",
      "step 10000, train accuracy 0.862, train loss 466.048248\n",
      "step 10200, train accuracy 0.869, train loss 486.232544\n",
      "step 10400, train accuracy 0.86, train loss 487.577881\n",
      "step 10600, train accuracy 0.875, train loss 452.089722\n",
      "step 10800, train accuracy 0.862, train loss 445.447144\n",
      "step 11000, train accuracy 0.902, train loss 389.457581\n",
      "step 11200, train accuracy 0.873, train loss 432.320251\n",
      "step 11400, train accuracy 0.89, train loss 379.958557\n",
      "step 11600, train accuracy 0.899, train loss 341.476166\n",
      "step 11800, train accuracy 0.907, train loss 392.301636\n",
      "step 12000, train accuracy 0.899, train loss 364.530853\n",
      "step 12200, train accuracy 0.89, train loss 362.629028\n",
      "step 12400, train accuracy 0.929, train loss 306.529053\n",
      "step 12600, train accuracy 0.916, train loss 326.231140\n",
      "step 12800, train accuracy 0.914, train loss 308.671173\n",
      "step 13000, train accuracy 0.92, train loss 319.273193\n",
      "step 13200, train accuracy 0.927, train loss 258.127441\n",
      "step 13400, train accuracy 0.94, train loss 266.418549\n",
      "step 13600, train accuracy 0.937, train loss 254.230377\n",
      "step 13800, train accuracy 0.937, train loss 244.148071\n",
      "step 14000, train accuracy 0.934, train loss 247.161896\n",
      "step 14200, train accuracy 0.947, train loss 222.287994\n",
      "step 14400, train accuracy 0.946, train loss 244.090210\n",
      "step 14600, train accuracy 0.948, train loss 210.810211\n",
      "step 14800, train accuracy 0.946, train loss 230.517334\n",
      "step 15000, train accuracy 0.94, train loss 215.464478\n",
      "step 15200, train accuracy 0.97, train loss 179.753571\n",
      "step 15400, train accuracy 0.958, train loss 196.178726\n",
      "step 15600, train accuracy 0.966, train loss 190.990067\n",
      "step 15800, train accuracy 0.96, train loss 187.177856\n",
      "step 16000, train accuracy 0.963, train loss 195.135376\n",
      "step 16200, train accuracy 0.964, train loss 167.935837\n",
      "step 16400, train accuracy 0.977, train loss 145.862000\n",
      "step 16600, train accuracy 0.957, train loss 181.123550\n",
      "step 16800, train accuracy 0.981, train loss 122.024490\n",
      "step 17000, train accuracy 0.962, train loss 145.772110\n",
      "step 17200, train accuracy 0.976, train loss 134.795013\n",
      "step 17400, train accuracy 0.979, train loss 117.521179\n",
      "step 17600, train accuracy 0.971, train loss 142.251801\n",
      "step 17800, train accuracy 0.978, train loss 120.986786\n",
      "step 18000, train accuracy 0.982, train loss 148.697220\n",
      "step 18200, train accuracy 0.978, train loss 117.984940\n",
      "step 18400, train accuracy 0.982, train loss 142.164734\n",
      "step 18600, train accuracy 0.985, train loss 100.084435\n",
      "step 18800, train accuracy 0.975, train loss 139.467957\n",
      "step 19000, train accuracy 0.989, train loss 94.529922\n",
      "step 19200, train accuracy 0.988, train loss 97.310646\n",
      "step 19400, train accuracy 0.99, train loss 80.395111\n",
      "step 19600, train accuracy 0.982, train loss 86.333366\n",
      "step 19800, train accuracy 0.993, train loss 83.396187\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/data.npy'\n",
    "\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 18, 64])\n",
    "b_conv1_1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv1_2 = bias_variable([64])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_1 = bias_variable([64])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([8*8*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 17])\n",
    "b_fc2 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 1000\n",
    "for i in range(20000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i,  train_accuracy, loss_temp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63880002"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training.npy')\n",
    "accuracy.eval(feed_dict={x:vali[:,:-17], y_:vali[:,-17:], keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is  (24119, 18449)\n",
      "step 0, train accuracy 0.092, train loss 14185.763672\n",
      "step 200, train accuracy 0.318, train loss 10909.431641\n",
      "step 400, train accuracy 0.392, train loss 9721.892578\n",
      "step 600, train accuracy 0.422, train loss 9072.408203\n",
      "step 800, train accuracy 0.509, train loss 7472.146973\n",
      "step 1000, train accuracy 0.554, train loss 6482.962402\n",
      "step 1200, train accuracy 0.444, train loss 2385.441895\n",
      "step 1400, train accuracy 0.488, train loss 1975.648926\n",
      "step 1600, train accuracy 0.51, train loss 1806.054321\n",
      "step 1800, train accuracy 0.555, train loss 1560.243408\n",
      "step 2000, train accuracy 0.552, train loss 1708.118408\n",
      "step 2200, train accuracy 0.558, train loss 1489.187256\n",
      "step 2400, train accuracy 0.566, train loss 1452.329224\n",
      "step 2600, train accuracy 0.584, train loss 1370.277588\n",
      "step 2800, train accuracy 0.592, train loss 1324.396729\n",
      "step 3000, train accuracy 0.592, train loss 1411.680054\n",
      "step 3200, train accuracy 0.635, train loss 1323.697144\n",
      "step 3400, train accuracy 0.647, train loss 1210.982178\n",
      "step 3600, train accuracy 0.629, train loss 1308.227905\n",
      "step 3800, train accuracy 0.64, train loss 1216.023804\n",
      "step 4000, train accuracy 0.651, train loss 1132.274536\n",
      "step 4200, train accuracy 0.681, train loss 1066.556885\n",
      "step 4400, train accuracy 0.677, train loss 1122.683472\n",
      "step 4600, train accuracy 0.691, train loss 1059.245850\n",
      "step 4800, train accuracy 0.696, train loss 1056.686890\n",
      "step 5000, train accuracy 0.703, train loss 1073.073975\n",
      "step 5200, train accuracy 0.694, train loss 1022.036560\n",
      "step 5400, train accuracy 0.688, train loss 1008.140137\n",
      "step 5600, train accuracy 0.724, train loss 920.812988\n",
      "step 5800, train accuracy 0.728, train loss 873.026978\n",
      "step 6000, train accuracy 0.712, train loss 943.207153\n",
      "step 6200, train accuracy 0.745, train loss 885.425781\n",
      "step 6400, train accuracy 0.745, train loss 901.910156\n",
      "step 6600, train accuracy 0.778, train loss 902.130493\n",
      "step 6800, train accuracy 0.759, train loss 832.685303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-73d16ecb41a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mloss_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m     \"\"\"\n\u001b[1;32m--> 707\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5211\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5212\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5213\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                              \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "filename = 'E:/Alibaba German AI Challenge/data_process/vali_data.npy'\n",
    "\n",
    "data = np.load(filename)\n",
    "print('The shape of data is ',data.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 18432])\n",
    "y_ = tf.placeholder(\"float\", [None, 17])\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 18])\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 18, 64])\n",
    "b_conv1_1 = bias_variable([64])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 64, 128])\n",
    "b_conv1_2 = bias_variable([128])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 128, 128])\n",
    "b_conv2_1 = bias_variable([128])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 128, 128])\n",
    "b_conv2_2 = bias_variable([128])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "# fc_1\n",
    "w_fc1 = weight_variable([8*8*128, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout_1\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# fc_2\n",
    "w_fc2 = weight_variable([1024, 1024])\n",
    "b_fc2 = bias_variable([1024])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# dropout_2\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc3 = weight_variable([1024, 17])\n",
    "b_fc3 = bias_variable([17])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc2_drop, w_fc3) + b_fc3)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "\n",
    "##随机抽取一部分作为一个mini-batch\n",
    "def get_batch(data, batch_size):\n",
    "    sample = random.sample(list(data),batch_size)\n",
    "    sample = np.array(sample)\n",
    "    train_x = sample[:,:-17]\n",
    "    train_y = sample[:,-17:]\n",
    "    \n",
    "    return train_x, train_y\n",
    "\n",
    "Loss = []\n",
    "\n",
    "batch_size = 1000\n",
    "for i in range(30000):\n",
    "    batch_x,batch_y = get_batch(data,batch_size)\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n",
    "    loss_temp = cross_entropy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "    Loss.append(loss_temp)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g, train loss %f\" %(i,  train_accuracy, loss_temp))\n",
    "        if train_accuracy >= 0.99:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.542"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali = np.load('E:/Alibaba German AI Challenge/data_process/sample_of_training.npy')\n",
    "accuracy.eval(feed_dict={x:vali[:1000,:-17], y_:vali[:1000,-17:], keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAE/CAYAAACqxdFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2170e80d0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xx = list(range(len(Loss)))\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(xx,Loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "1   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "2   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "4   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'E:/Alibaba German AI Challenge/origin_DATA/round1_test_a_20181109.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "test_s1 = f['sen1']\n",
    "test_s2 = f['sen2']\n",
    "\n",
    "test = []\n",
    "for i in range(0,test_s1.shape[0]):\n",
    "    temp1 = test_s1[i].flatten()\n",
    "    temp2 = test_s2[i].flatten()\n",
    "    temp = np.hstack((temp1,temp2))\n",
    "    test.append(temp)\n",
    "test = np.array(test)\n",
    "\n",
    "test_y = np.zeros((test.shape[0],17))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "one_hot=tf.one_hot(P,17)\n",
    "Pred_one_hot = sess.run(one_hot)\n",
    "Pred_one_hot = Pred_one_hot.astype(np.int32)\n",
    "out = pd.DataFrame(Pred_one_hot, columns = list(range(17)))\n",
    "print(out.head())\n",
    "\n",
    "out.to_csv('fifth_deepCNN_7k_batch_vali_Adam.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
